{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "imagewidth = 800\n",
    "plot_size = (21, 14)\n",
    "half_plot_size = (18, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import math\n",
    "from math import log \n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import load_wine\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from IPython.display import display                               \n",
    "from ipywidgets import interactive\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "from sklearn.metrics import f1_score\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from ipywidgets import interact, fixed\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Change style according to FH\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "plt.style.use('/Users/dboehnke/Workspaces/20190307_FHKiel_Plotstyle/fh_kiel.mplstyle')\n",
    "\n",
    "from palettable.colorbrewer.qualitative import Paired_8 as q_map\n",
    "\n",
    "plt.set_cmap(q_map.mpl_colormap)\n",
    "filled_markers = ('o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X')\n",
    "\n",
    "blue = '#00305D'\n",
    "green = '#006A4D'\n",
    "orange = '#F49E00'\n",
    "dark_red = '#B5123E'\n",
    "alarm_red = '#E20020'\n",
    "light_green = '#7AB51D'\n",
    "\n",
    "cmap_kiel = ListedColormap(['#00305D', '#F49E00', '#006A4D'])\n",
    "cmap_kiel2 = ListedColormap(['#00305D', '#F49E00'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Grundlagen Maschinelles Lernen\n",
    "\n",
    "## Fundamentals of Machine Learning\n",
    "\n",
    "\n",
    "Prof. Dr.-Ing. Daniel Böhnke\n",
    "\n",
    "<div style=\"text-align: right\"> Woche 12 v1 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vorhersage des Modells\n",
    "\n",
    "<center><img src=\"img/nn_feedforward.png\" width=40% class=\"stretch\"></center>\n",
    "\n",
    "* Für die Vorhersage des Modells wird vom Input zum Output gerechnet. \n",
    "* Für ein einzelnes Neuron bedeuted dies, dass alle Inputs mal den Gewichten summiert werden und mit dem Bias in die Aktivierungsfunktion eingehen.\n",
    "\n",
    "$$y_i = \\sigma(\\sum{w_i x_i} + b)$$\n",
    "\n",
    "* Ein mögliches Beispiel für eine Aktivierungsfunktion ist die Sigmoid Funktion:\n",
    "\n",
    "$$s(x) = \\frac{1}{1+e^x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fehler des Modells\n",
    "\n",
    "<center><img src=\"img/nn_cost.png\" width=40% class=\"stretch\"></center>\n",
    "\n",
    "* Neuronale Netze können für eine Vielzahl von Aufgaben eingesetzt werden. \n",
    "* Im Supervised Learning können Klassifikations- & Regressionsaufgaben umgesetzt werden. \n",
    "* Entsprechend der Fragestellung wird eine Kosten-Funktion festgelegt, z.B. MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Prinzip Backpropagation\n",
    "\n",
    "<center><img src=\"img/nn_backpropagation.png\" width=40% class=\"stretch\"></center>\n",
    "\n",
    "* Die interessante Frage lautet nun: Wie müssen die Gewichte und der Bias angepasst werden, um die Kosten Funktion zu minimieren. \n",
    "* Der dazugehörige Algorithmus heißt **Backpropagation**. Er basiert auf dem negativen Gradienten der Kosten-Funktion. \n",
    "* **Backpropagation** ist der entscheidende Aspekt für das Training von neuronalen Netzen!\n",
    "* *Cells that fire together wire together*, nach D. Hebb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Convolutional Layer\n",
    "\n",
    "\n",
    "<center><img src=\"img/conv.gif\" width=30% class=\"stretch\"></center>\n",
    "\n",
    "\n",
    "* Die Neuronalen Netze sind nicht mehr ein- sondern mehr-dimensional\n",
    "* Wir betrachten nicht den Inhalt als Ganzes sondern überlappende Bereiche\n",
    "* Analogie zu einem Filter\n",
    "\n",
    "* Analogie zum biologischen Sehen\n",
    "    * Erkennen von verschiedenen Mustern\n",
    "    * Einzelne Muster kombinieren sich zu komplexeren Mustern\n",
    "    * Absolute Position von Inhalten ist selten relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Heute\n",
    "\n",
    "* Künstliche Neuronale Netze sind ein weiterer Modellansatz aus dem maschinellen Lernen.\n",
    "* Im Zuge des Deep-Learnings haben diese Netze extreme Popularität gewonnen und werden für viele neue Ansätze erfolgreich angewendet.\n",
    "* Der Ruhm ist sicher gerechtfertigt, die klassischen Ansätze haben aber weiter Ihre Daseinsberechtigung. \n",
    "* Heute wird eine (sehr) kurze Einführung in ANN erfolgen.\n",
    "* In der kommenden Woche werden einige der neueren Lernansätze vorgestellt, die mittels ANN möglich sind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Lernlandkarte \n",
    "<center><img src=\"img/lernlandkarte.png\" width=80% class=\"stretch\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Autoencoders\n",
    "\n",
    "* Ein Autoencoder ist eine Form eines künstlichen neuronalen Netzes, welches versucht die Eingangsinformationen zu komprimieren und mit den reduzierten Informationen im Ausgang wieder korrekt nachzubilden. \n",
    "- **Struktur**:\n",
    "  - **Encoder**: Nimmt Eingabedaten und komprimiert sie zu einer niedrigdimensionalen Darstellung.\n",
    "  - **Latent Space**: Der komprimierte, niedrigdimensionale Raum, der die wesentlichen Merkmale der Eingabedaten enthält.\n",
    "  - **Decoder**: Rekonstruiert die Eingabedaten aus der niedrigdimensionalen Darstellung.\n",
    "\n",
    "<img src=\"img/autoencoder_schema.jpeg\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Encoder\n",
    "* Reduziert die Dimensionen der Eingangsinformationen (auch Dimensionsreduktion)\n",
    "* Informationen werden auf das wichtigste/ den Durchschnitt Reduziert\n",
    "* Im KNN: Reduzierung von Knotenpunkten\n",
    "<img src=\"img/encoder.png\" style=\"width: 50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Decoder\n",
    "* Rekonstruiert mithilfe der komprimierten Informationen die ursprünglichen Daten\n",
    "* Im KNN: Vervielfachung von Knotenpunkten (auf Ursprungsgröße)\n",
    "<img src=\"img/decoder.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Autoencoder Beispiel Fan Anamoly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>zcr</th>\n",
       "      <th>energy</th>\n",
       "      <th>energy_entropy</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_spread</th>\n",
       "      <th>spectral_entropy</th>\n",
       "      <th>spectral_flux</th>\n",
       "      <th>spectral_rolloff</th>\n",
       "      <th>mfcc_1</th>\n",
       "      <th>...</th>\n",
       "      <th>chroma_5</th>\n",
       "      <th>chroma_6</th>\n",
       "      <th>chroma_7</th>\n",
       "      <th>chroma_8</th>\n",
       "      <th>chroma_9</th>\n",
       "      <th>chroma_10</th>\n",
       "      <th>chroma_11</th>\n",
       "      <th>chroma_12</th>\n",
       "      <th>chroma_std</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.145940</td>\n",
       "      <td>3.263259</td>\n",
       "      <td>0.163774</td>\n",
       "      <td>0.215936</td>\n",
       "      <td>0.256938</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.057489</td>\n",
       "      <td>-21.253260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.003313</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.011340</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>0.036758</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>0.173939</td>\n",
       "      <td>3.253800</td>\n",
       "      <td>0.152841</td>\n",
       "      <td>0.208557</td>\n",
       "      <td>0.249979</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>-20.981265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.008602</td>\n",
       "      <td>0.001773</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>0.012540</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.033128</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.040091</td>\n",
       "      <td>0.138430</td>\n",
       "      <td>3.173616</td>\n",
       "      <td>0.147057</td>\n",
       "      <td>0.206017</td>\n",
       "      <td>0.243154</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>-21.310778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003587</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.008814</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.028370</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.039334</td>\n",
       "      <td>0.169484</td>\n",
       "      <td>3.200191</td>\n",
       "      <td>0.133746</td>\n",
       "      <td>0.182948</td>\n",
       "      <td>0.257049</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>-21.081264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.000283</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.023649</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.035072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.046142</td>\n",
       "      <td>0.133602</td>\n",
       "      <td>3.205563</td>\n",
       "      <td>0.136393</td>\n",
       "      <td>0.182847</td>\n",
       "      <td>0.319426</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.065053</td>\n",
       "      <td>-21.036903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.009597</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.006384</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.008966</td>\n",
       "      <td>0.021396</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.024279</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1581</th>\n",
       "      <td>1972</td>\n",
       "      <td>0.050681</td>\n",
       "      <td>0.092525</td>\n",
       "      <td>3.198266</td>\n",
       "      <td>0.152644</td>\n",
       "      <td>0.193954</td>\n",
       "      <td>0.400336</td>\n",
       "      <td>0.001035</td>\n",
       "      <td>0.071104</td>\n",
       "      <td>-21.355534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.003971</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.009810</td>\n",
       "      <td>0.015273</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.022968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>1973</td>\n",
       "      <td>0.059002</td>\n",
       "      <td>0.093857</td>\n",
       "      <td>3.090540</td>\n",
       "      <td>0.147865</td>\n",
       "      <td>0.185864</td>\n",
       "      <td>0.428685</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.066566</td>\n",
       "      <td>-21.085921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.021588</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>1975</td>\n",
       "      <td>0.052194</td>\n",
       "      <td>0.099665</td>\n",
       "      <td>3.125861</td>\n",
       "      <td>0.149125</td>\n",
       "      <td>0.194021</td>\n",
       "      <td>0.345721</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.066566</td>\n",
       "      <td>-21.247526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.002977</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>0.006471</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.028206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>1980</td>\n",
       "      <td>0.042360</td>\n",
       "      <td>0.135078</td>\n",
       "      <td>3.231218</td>\n",
       "      <td>0.133902</td>\n",
       "      <td>0.176379</td>\n",
       "      <td>0.250892</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.055976</td>\n",
       "      <td>-21.264691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>0.036033</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>1982</td>\n",
       "      <td>0.037821</td>\n",
       "      <td>0.175911</td>\n",
       "      <td>3.216357</td>\n",
       "      <td>0.140143</td>\n",
       "      <td>0.183246</td>\n",
       "      <td>0.275176</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.054463</td>\n",
       "      <td>-20.769723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.015793</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.016167</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.030076</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1162 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       zcr    energy  energy_entropy  spectral_centroid  \\\n",
       "0        0  0.044629  0.145940        3.263259           0.163774   \n",
       "1        2  0.027988  0.173939        3.253800           0.152841   \n",
       "2        3  0.040091  0.138430        3.173616           0.147057   \n",
       "3        4  0.039334  0.169484        3.200191           0.133746   \n",
       "4        6  0.046142  0.133602        3.205563           0.136393   \n",
       "...    ...       ...       ...             ...                ...   \n",
       "1581  1972  0.050681  0.092525        3.198266           0.152644   \n",
       "1582  1973  0.059002  0.093857        3.090540           0.147865   \n",
       "1583  1975  0.052194  0.099665        3.125861           0.149125   \n",
       "1584  1980  0.042360  0.135078        3.231218           0.133902   \n",
       "1585  1982  0.037821  0.175911        3.216357           0.140143   \n",
       "\n",
       "      spectral_spread  spectral_entropy  spectral_flux  spectral_rolloff  \\\n",
       "0            0.215936          0.256938       0.008492          0.057489   \n",
       "1            0.208557          0.249979       0.000975          0.054463   \n",
       "2            0.206017          0.243154       0.000844          0.054463   \n",
       "3            0.182948          0.257049       0.000997          0.054463   \n",
       "4            0.182847          0.319426       0.001855          0.065053   \n",
       "...               ...               ...            ...               ...   \n",
       "1581         0.193954          0.400336       0.001035          0.071104   \n",
       "1582         0.185864          0.428685       0.001133          0.066566   \n",
       "1583         0.194021          0.345721       0.000659          0.066566   \n",
       "1584         0.176379          0.250892       0.001046          0.055976   \n",
       "1585         0.183246          0.275176       0.001195          0.054463   \n",
       "\n",
       "         mfcc_1  ...  chroma_5  chroma_6  chroma_7  chroma_8  chroma_9  \\\n",
       "0    -21.253260  ...  0.001284  0.003313  0.002948  0.004459  0.001922   \n",
       "1    -20.981265  ...  0.001194  0.003140  0.001478  0.008602  0.001773   \n",
       "2    -21.310778  ...  0.003587  0.005918  0.001774  0.008814  0.000727   \n",
       "3    -21.081264  ...  0.002269  0.003672  0.001897  0.008451  0.000283   \n",
       "4    -21.036903  ...  0.001515  0.009597  0.001146  0.006384  0.001270   \n",
       "...         ...  ...       ...       ...       ...       ...       ...   \n",
       "1581 -21.355534  ...  0.002249  0.003971  0.001502  0.017530  0.004194   \n",
       "1582 -21.085921  ...  0.003535  0.009857  0.006595  0.027813  0.007793   \n",
       "1583 -21.247526  ...  0.002047  0.002977  0.002615  0.014037  0.003881   \n",
       "1584 -21.264691  ...  0.002500  0.003307  0.003113  0.007273  0.002701   \n",
       "1585 -20.769723  ...  0.001885  0.006645  0.002554  0.015793  0.001034   \n",
       "\n",
       "      chroma_10  chroma_11  chroma_12  chroma_std  anomaly  \n",
       "0      0.004207   0.011340   0.000972    0.036758      0.0  \n",
       "1      0.003683   0.012540   0.001376    0.033128      0.0  \n",
       "2      0.003828   0.016074   0.001290    0.028370      0.0  \n",
       "3      0.002932   0.023649   0.002799    0.035072      0.0  \n",
       "4      0.008966   0.021396   0.001724    0.024279      0.0  \n",
       "...         ...        ...        ...         ...      ...  \n",
       "1581   0.009810   0.015273   0.003622    0.022968      0.0  \n",
       "1582   0.010026   0.021588   0.002716    0.015797      0.0  \n",
       "1583   0.007336   0.006471   0.001132    0.028206      0.0  \n",
       "1584   0.004701   0.003652   0.003301    0.036033      0.0  \n",
       "1585   0.004282   0.016167   0.001704    0.030076      0.0  \n",
       "\n",
       "[1162 rows x 36 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df  = pd.read_csv(\"./data/fan_anomaly_train.csv\", delimiter=\",\")\n",
    "df = df[df[\"anomaly\"] == 0.0]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Skalierung  / Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07142857,  0.66187554,  0.75138244, ..., -0.70251797,\n",
       "         0.98678161,  0.        ],\n",
       "       [-1.11904762,  1.3756604 ,  0.61492788, ..., -0.4900994 ,\n",
       "         0.55473689,  0.        ],\n",
       "       [-0.35714286,  0.47043178, -0.54182733, ..., -0.53554431,\n",
       "        -0.01153658,  0.        ],\n",
       "       ...,\n",
       "       [ 0.4047619 , -0.51778947, -1.23074075, ..., -0.6183874 ,\n",
       "        -0.03101826,  0.        ],\n",
       "       [-0.21428571,  0.38497853,  0.28915984, ...,  0.51999136,\n",
       "         0.90054488,  0.        ],\n",
       "       [-0.5       ,  1.42592508,  0.07477348, ..., -0.31814992,\n",
       "         0.1914626 ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "scaler = RobustScaler()  # or MinMaxScaler()\n",
    "df = scaler.fit_transform(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Definiere Netze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=35, hidden_size=18, encoding_dim=6):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, input_size*4//5),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(input_size*4//5, input_size*3//5),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(input_size*3//5, input_size*2//5),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(input_size*2//5, encoding_dim),\n",
    "            nn.LeakyReLU(0.001)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_size*2//5),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(input_size*2//5, input_size*3//5),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(input_size*3//5, input_size*4//5),\n",
    "            nn.LeakyReLU(0.001),\n",
    "            nn.Linear(input_size*4//5, input_size),\n",
    "            nn.LeakyReLU(0.001)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Trainiere Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n"
     ]
    }
   ],
   "source": [
    "print(\"Load Data\")    \n",
    "your_data = torch.tensor(df) \n",
    "your_data = your_data.to(torch.float32)\n",
    "dataset = CustomDataset(your_data)\n",
    "data_loader = data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Model\n"
     ]
    }
   ],
   "source": [
    "print(\"Init Model\")    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.MSELoss()#nn.SmoothL1Loss()#\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch [1/2000], Loss: 1.2833\n",
      "Epoch [2/2000], Loss: 1.2420\n",
      "Epoch [3/2000], Loss: 0.9424\n",
      "Epoch [4/2000], Loss: 0.7275\n",
      "Epoch [5/2000], Loss: 0.6482\n",
      "Epoch [6/2000], Loss: 0.6127\n",
      "Epoch [7/2000], Loss: 0.5861\n",
      "Epoch [8/2000], Loss: 0.5730\n",
      "Epoch [9/2000], Loss: 0.5760\n",
      "Epoch [10/2000], Loss: 0.5522\n",
      "Epoch [11/2000], Loss: 0.5476\n",
      "Epoch [12/2000], Loss: 0.5405\n",
      "Epoch [13/2000], Loss: 0.5386\n",
      "Epoch [14/2000], Loss: 0.5330\n",
      "Epoch [15/2000], Loss: 0.5274\n",
      "Epoch [16/2000], Loss: 0.5321\n",
      "Epoch [17/2000], Loss: 0.5267\n",
      "Epoch [18/2000], Loss: 0.5161\n",
      "Epoch [19/2000], Loss: 0.5123\n",
      "Epoch [20/2000], Loss: 0.5137\n",
      "Epoch [21/2000], Loss: 0.5123\n",
      "Epoch [22/2000], Loss: 0.5047\n",
      "Epoch [23/2000], Loss: 0.5010\n",
      "Epoch [24/2000], Loss: 0.5017\n",
      "Epoch [25/2000], Loss: 0.5143\n",
      "Epoch [26/2000], Loss: 0.5076\n",
      "Epoch [27/2000], Loss: 0.5376\n",
      "Epoch [28/2000], Loss: 0.4964\n",
      "Epoch [29/2000], Loss: 0.4944\n",
      "Epoch [30/2000], Loss: 0.4929\n",
      "Epoch [31/2000], Loss: 0.4961\n",
      "Epoch [32/2000], Loss: 0.5084\n",
      "Epoch [33/2000], Loss: 0.4930\n",
      "Epoch [34/2000], Loss: 0.4866\n",
      "Epoch [35/2000], Loss: 0.4836\n",
      "Epoch [36/2000], Loss: 0.4830\n",
      "Epoch [37/2000], Loss: 0.4810\n",
      "Epoch [38/2000], Loss: 0.4808\n",
      "Epoch [39/2000], Loss: 0.4791\n",
      "Epoch [40/2000], Loss: 0.4791\n",
      "Epoch [41/2000], Loss: 0.4776\n",
      "Epoch [42/2000], Loss: 0.4796\n",
      "Epoch [43/2000], Loss: 0.4763\n",
      "Epoch [44/2000], Loss: 0.4774\n",
      "Epoch [45/2000], Loss: 0.4894\n",
      "Epoch [46/2000], Loss: 0.4755\n",
      "Epoch [47/2000], Loss: 0.4774\n",
      "Epoch [48/2000], Loss: 0.4757\n",
      "Epoch [49/2000], Loss: 0.4769\n",
      "Epoch [50/2000], Loss: 0.4929\n",
      "Epoch [51/2000], Loss: 0.4788\n",
      "Epoch [52/2000], Loss: 0.4714\n",
      "Epoch [53/2000], Loss: 0.4730\n",
      "Epoch [54/2000], Loss: 0.4717\n",
      "Epoch [55/2000], Loss: 0.4732\n",
      "Epoch [56/2000], Loss: 0.4709\n",
      "Epoch [57/2000], Loss: 0.4689\n",
      "Epoch [58/2000], Loss: 0.4698\n",
      "Epoch [59/2000], Loss: 0.4771\n",
      "Epoch [60/2000], Loss: 0.4678\n",
      "Epoch [61/2000], Loss: 0.4667\n",
      "Epoch [62/2000], Loss: 0.4679\n",
      "Epoch [63/2000], Loss: 0.4671\n",
      "Epoch [64/2000], Loss: 0.4666\n",
      "Epoch [65/2000], Loss: 0.4692\n",
      "Epoch [66/2000], Loss: 0.4678\n",
      "Epoch [67/2000], Loss: 0.4748\n",
      "Epoch [68/2000], Loss: 0.4668\n",
      "Epoch [69/2000], Loss: 0.4668\n",
      "Epoch [70/2000], Loss: 0.4651\n",
      "Epoch [71/2000], Loss: 0.4707\n",
      "Epoch [72/2000], Loss: 0.4661\n",
      "Epoch [73/2000], Loss: 0.4650\n",
      "Epoch [74/2000], Loss: 0.4653\n",
      "Epoch [75/2000], Loss: 0.4688\n",
      "Epoch [76/2000], Loss: 0.4833\n",
      "Epoch [77/2000], Loss: 0.4748\n",
      "Epoch [78/2000], Loss: 0.4657\n",
      "Epoch [79/2000], Loss: 0.4631\n",
      "Epoch [80/2000], Loss: 0.4643\n",
      "Epoch [81/2000], Loss: 0.4621\n",
      "Epoch [82/2000], Loss: 0.4625\n",
      "Epoch [83/2000], Loss: 0.4645\n",
      "Epoch [84/2000], Loss: 0.4654\n",
      "Epoch [85/2000], Loss: 0.4683\n",
      "Epoch [86/2000], Loss: 0.4626\n",
      "Epoch [87/2000], Loss: 0.4614\n",
      "Epoch [88/2000], Loss: 0.4605\n",
      "Epoch [89/2000], Loss: 0.4593\n",
      "Epoch [90/2000], Loss: 0.4601\n",
      "Epoch [91/2000], Loss: 0.4634\n",
      "Epoch [92/2000], Loss: 0.4613\n",
      "Epoch [93/2000], Loss: 0.4634\n",
      "Epoch [94/2000], Loss: 0.4615\n",
      "Epoch [95/2000], Loss: 0.4610\n",
      "Epoch [96/2000], Loss: 0.4607\n",
      "Epoch [97/2000], Loss: 0.4619\n",
      "Epoch [98/2000], Loss: 0.4625\n",
      "Epoch [99/2000], Loss: 0.4598\n",
      "Epoch [100/2000], Loss: 0.4595\n",
      "Epoch [101/2000], Loss: 0.4612\n",
      "Epoch [102/2000], Loss: 0.4631\n",
      "Epoch [103/2000], Loss: 0.4597\n",
      "Epoch [104/2000], Loss: 0.4592\n",
      "Epoch [105/2000], Loss: 0.4600\n",
      "Epoch [106/2000], Loss: 0.4561\n",
      "Epoch [107/2000], Loss: 0.4571\n",
      "Epoch [108/2000], Loss: 0.4727\n",
      "Epoch [109/2000], Loss: 0.4623\n",
      "Epoch [110/2000], Loss: 0.4603\n",
      "Epoch [111/2000], Loss: 0.4583\n",
      "Epoch [112/2000], Loss: 0.4599\n",
      "Epoch [113/2000], Loss: 0.4617\n",
      "Epoch [114/2000], Loss: 0.4750\n",
      "Epoch [115/2000], Loss: 0.4565\n",
      "Epoch [116/2000], Loss: 0.4576\n",
      "Epoch [117/2000], Loss: 0.4545\n",
      "Epoch [118/2000], Loss: 0.4535\n",
      "Epoch [119/2000], Loss: 0.4542\n",
      "Epoch [120/2000], Loss: 0.4548\n",
      "Epoch [121/2000], Loss: 0.4552\n",
      "Epoch [122/2000], Loss: 0.4564\n",
      "Epoch [123/2000], Loss: 0.4569\n",
      "Epoch [124/2000], Loss: 0.4537\n",
      "Epoch [125/2000], Loss: 0.4542\n",
      "Epoch [126/2000], Loss: 0.4539\n",
      "Epoch [127/2000], Loss: 0.4566\n",
      "Epoch [128/2000], Loss: 0.4559\n",
      "Epoch [129/2000], Loss: 0.4535\n",
      "Epoch [130/2000], Loss: 0.4533\n",
      "Epoch [131/2000], Loss: 0.4570\n",
      "Epoch [132/2000], Loss: 0.4560\n",
      "Epoch [133/2000], Loss: 0.4578\n",
      "Epoch [134/2000], Loss: 0.4617\n",
      "Epoch [135/2000], Loss: 0.4663\n",
      "Epoch [136/2000], Loss: 0.4691\n",
      "Epoch [137/2000], Loss: 0.4520\n",
      "Epoch [138/2000], Loss: 0.4514\n",
      "Epoch [139/2000], Loss: 0.4507\n",
      "Epoch [140/2000], Loss: 0.4505\n",
      "Epoch [141/2000], Loss: 0.4499\n",
      "Epoch [142/2000], Loss: 0.4501\n",
      "Epoch [143/2000], Loss: 0.4515\n",
      "Epoch [144/2000], Loss: 0.4555\n",
      "Epoch [145/2000], Loss: 0.4547\n",
      "Epoch [146/2000], Loss: 0.4513\n",
      "Epoch [147/2000], Loss: 0.4485\n",
      "Epoch [148/2000], Loss: 0.4546\n",
      "Epoch [149/2000], Loss: 0.4542\n",
      "Epoch [150/2000], Loss: 0.4568\n",
      "Epoch [151/2000], Loss: 0.4513\n",
      "Epoch [152/2000], Loss: 0.4491\n",
      "Epoch [153/2000], Loss: 0.4505\n",
      "Epoch [154/2000], Loss: 0.4525\n",
      "Epoch [155/2000], Loss: 0.4501\n",
      "Epoch [156/2000], Loss: 0.4566\n",
      "Epoch [157/2000], Loss: 0.4505\n",
      "Epoch [158/2000], Loss: 0.4492\n",
      "Epoch [159/2000], Loss: 0.4479\n",
      "Epoch [160/2000], Loss: 0.4475\n",
      "Epoch [161/2000], Loss: 0.4470\n",
      "Epoch [162/2000], Loss: 0.4468\n",
      "Epoch [163/2000], Loss: 0.4455\n",
      "Epoch [164/2000], Loss: 0.4470\n",
      "Epoch [165/2000], Loss: 0.4468\n",
      "Epoch [166/2000], Loss: 0.4476\n",
      "Epoch [167/2000], Loss: 0.4495\n",
      "Epoch [168/2000], Loss: 0.4456\n",
      "Epoch [169/2000], Loss: 0.4462\n",
      "Epoch [170/2000], Loss: 0.4484\n",
      "Epoch [171/2000], Loss: 0.4500\n",
      "Epoch [172/2000], Loss: 0.4457\n",
      "Epoch [173/2000], Loss: 0.4477\n",
      "Epoch [174/2000], Loss: 0.4479\n",
      "Epoch [175/2000], Loss: 0.4453\n",
      "Epoch [176/2000], Loss: 0.4486\n",
      "Epoch [177/2000], Loss: 0.4476\n",
      "Epoch [178/2000], Loss: 0.4432\n",
      "Epoch [179/2000], Loss: 0.4430\n",
      "Epoch [180/2000], Loss: 0.4580\n",
      "Epoch [181/2000], Loss: 0.4479\n",
      "Epoch [182/2000], Loss: 0.4476\n",
      "Epoch [183/2000], Loss: 0.4490\n",
      "Epoch [184/2000], Loss: 0.4636\n",
      "Epoch [185/2000], Loss: 0.4917\n",
      "Epoch [186/2000], Loss: 0.4702\n",
      "Epoch [187/2000], Loss: 0.4561\n",
      "Epoch [188/2000], Loss: 0.4464\n",
      "Epoch [189/2000], Loss: 0.4454\n",
      "Epoch [190/2000], Loss: 0.4444\n",
      "Epoch [191/2000], Loss: 0.4416\n",
      "Epoch [192/2000], Loss: 0.4411\n",
      "Epoch [193/2000], Loss: 0.4410\n",
      "Epoch [194/2000], Loss: 0.4419\n",
      "Epoch [195/2000], Loss: 0.4407\n",
      "Epoch [196/2000], Loss: 0.4406\n",
      "Epoch [197/2000], Loss: 0.4411\n",
      "Epoch [198/2000], Loss: 0.4407\n",
      "Epoch [199/2000], Loss: 0.4395\n",
      "Epoch [200/2000], Loss: 0.4414\n",
      "Epoch [201/2000], Loss: 0.4409\n",
      "Epoch [202/2000], Loss: 0.4392\n",
      "Epoch [203/2000], Loss: 0.4394\n",
      "Epoch [204/2000], Loss: 0.4433\n",
      "Epoch [205/2000], Loss: 0.4454\n",
      "Epoch [206/2000], Loss: 0.4398\n",
      "Epoch [207/2000], Loss: 0.4392\n",
      "Epoch [208/2000], Loss: 0.4401\n",
      "Epoch [209/2000], Loss: 0.4388\n",
      "Epoch [210/2000], Loss: 0.4380\n",
      "Epoch [211/2000], Loss: 0.4407\n",
      "Epoch [212/2000], Loss: 0.4405\n",
      "Epoch [213/2000], Loss: 0.4380\n",
      "Epoch [214/2000], Loss: 0.4395\n",
      "Epoch [215/2000], Loss: 0.4389\n",
      "Epoch [216/2000], Loss: 0.4398\n",
      "Epoch [217/2000], Loss: 0.4418\n",
      "Epoch [218/2000], Loss: 0.4455\n",
      "Epoch [219/2000], Loss: 0.4445\n",
      "Epoch [220/2000], Loss: 0.4366\n",
      "Epoch [221/2000], Loss: 0.4402\n",
      "Epoch [222/2000], Loss: 0.4416\n",
      "Epoch [223/2000], Loss: 0.4414\n",
      "Epoch [224/2000], Loss: 0.4398\n",
      "Epoch [225/2000], Loss: 0.4389\n",
      "Epoch [226/2000], Loss: 0.4371\n",
      "Epoch [227/2000], Loss: 0.4438\n",
      "Epoch [228/2000], Loss: 0.4413\n",
      "Epoch [229/2000], Loss: 0.4354\n",
      "Epoch [230/2000], Loss: 0.4355\n",
      "Epoch [231/2000], Loss: 0.4367\n",
      "Epoch [232/2000], Loss: 0.4374\n",
      "Epoch [233/2000], Loss: 0.4382\n",
      "Epoch [234/2000], Loss: 0.4364\n",
      "Epoch [235/2000], Loss: 0.4371\n",
      "Epoch [236/2000], Loss: 0.4413\n",
      "Epoch [237/2000], Loss: 0.4462\n",
      "Epoch [238/2000], Loss: 0.4366\n",
      "Epoch [239/2000], Loss: 0.4362\n",
      "Epoch [240/2000], Loss: 0.4343\n",
      "Epoch [241/2000], Loss: 0.4429\n",
      "Epoch [242/2000], Loss: 0.4348\n",
      "Epoch [243/2000], Loss: 0.4353\n",
      "Epoch [244/2000], Loss: 0.4378\n",
      "Epoch [245/2000], Loss: 0.4367\n",
      "Epoch [246/2000], Loss: 0.4361\n",
      "Epoch [247/2000], Loss: 0.4335\n",
      "Epoch [248/2000], Loss: 0.4332\n",
      "Epoch [249/2000], Loss: 0.4329\n",
      "Epoch [250/2000], Loss: 0.4355\n",
      "Epoch [251/2000], Loss: 0.4367\n",
      "Epoch [252/2000], Loss: 0.4391\n",
      "Epoch [253/2000], Loss: 0.4340\n",
      "Epoch [254/2000], Loss: 0.4329\n",
      "Epoch [255/2000], Loss: 0.4342\n",
      "Epoch [256/2000], Loss: 0.4329\n",
      "Epoch [257/2000], Loss: 0.4379\n",
      "Epoch [258/2000], Loss: 0.4349\n",
      "Epoch [259/2000], Loss: 0.4319\n",
      "Epoch [260/2000], Loss: 0.4475\n",
      "Epoch [261/2000], Loss: 0.4374\n",
      "Epoch [262/2000], Loss: 0.4342\n",
      "Epoch [263/2000], Loss: 0.4320\n",
      "Epoch [264/2000], Loss: 0.4426\n",
      "Epoch [265/2000], Loss: 0.4395\n",
      "Epoch [266/2000], Loss: 0.4319\n",
      "Epoch [267/2000], Loss: 0.4332\n",
      "Epoch [268/2000], Loss: 0.4432\n",
      "Epoch [269/2000], Loss: 0.4356\n",
      "Epoch [270/2000], Loss: 0.4299\n",
      "Epoch [271/2000], Loss: 0.4285\n",
      "Epoch [272/2000], Loss: 0.4277\n",
      "Epoch [273/2000], Loss: 0.4278\n",
      "Epoch [274/2000], Loss: 0.4279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [275/2000], Loss: 0.4284\n",
      "Epoch [276/2000], Loss: 0.4301\n",
      "Epoch [277/2000], Loss: 0.4269\n",
      "Epoch [278/2000], Loss: 0.4259\n",
      "Epoch [279/2000], Loss: 0.4263\n",
      "Epoch [280/2000], Loss: 0.4296\n",
      "Epoch [281/2000], Loss: 0.4291\n",
      "Epoch [282/2000], Loss: 0.4241\n",
      "Epoch [283/2000], Loss: 0.4258\n",
      "Epoch [284/2000], Loss: 0.4273\n",
      "Epoch [285/2000], Loss: 0.4344\n",
      "Epoch [286/2000], Loss: 0.4298\n",
      "Epoch [287/2000], Loss: 0.4274\n",
      "Epoch [288/2000], Loss: 0.4245\n",
      "Epoch [289/2000], Loss: 0.4246\n",
      "Epoch [290/2000], Loss: 0.4311\n",
      "Epoch [291/2000], Loss: 0.4261\n",
      "Epoch [292/2000], Loss: 0.4379\n",
      "Epoch [293/2000], Loss: 0.4276\n",
      "Epoch [294/2000], Loss: 0.4246\n",
      "Epoch [295/2000], Loss: 0.4363\n",
      "Epoch [296/2000], Loss: 0.4263\n",
      "Epoch [297/2000], Loss: 0.4234\n",
      "Epoch [298/2000], Loss: 0.4229\n",
      "Epoch [299/2000], Loss: 0.4498\n",
      "Epoch [300/2000], Loss: 0.4241\n",
      "Epoch [301/2000], Loss: 0.4212\n",
      "Epoch [302/2000], Loss: 0.4207\n",
      "Epoch [303/2000], Loss: 0.4213\n",
      "Epoch [304/2000], Loss: 0.4208\n",
      "Epoch [305/2000], Loss: 0.4216\n",
      "Epoch [306/2000], Loss: 0.4208\n",
      "Epoch [307/2000], Loss: 0.4207\n",
      "Epoch [308/2000], Loss: 0.4236\n",
      "Epoch [309/2000], Loss: 0.4234\n",
      "Epoch [310/2000], Loss: 0.4196\n",
      "Epoch [311/2000], Loss: 0.4172\n",
      "Epoch [312/2000], Loss: 0.4185\n",
      "Epoch [313/2000], Loss: 0.4179\n",
      "Epoch [314/2000], Loss: 0.4168\n",
      "Epoch [315/2000], Loss: 0.4567\n",
      "Epoch [316/2000], Loss: 0.4472\n",
      "Epoch [317/2000], Loss: 0.4180\n",
      "Epoch [318/2000], Loss: 0.4169\n",
      "Epoch [319/2000], Loss: 0.4183\n",
      "Epoch [320/2000], Loss: 0.4174\n",
      "Epoch [321/2000], Loss: 0.4183\n",
      "Epoch [322/2000], Loss: 0.4217\n",
      "Epoch [323/2000], Loss: 0.4173\n",
      "Epoch [324/2000], Loss: 0.4169\n",
      "Epoch [325/2000], Loss: 0.4170\n",
      "Epoch [326/2000], Loss: 0.4149\n",
      "Epoch [327/2000], Loss: 0.4178\n",
      "Epoch [328/2000], Loss: 0.4130\n",
      "Epoch [329/2000], Loss: 0.4128\n",
      "Epoch [330/2000], Loss: 0.4117\n",
      "Epoch [331/2000], Loss: 0.4135\n",
      "Epoch [332/2000], Loss: 0.4136\n",
      "Epoch [333/2000], Loss: 0.4134\n",
      "Epoch [334/2000], Loss: 0.4137\n",
      "Epoch [335/2000], Loss: 0.4116\n",
      "Epoch [336/2000], Loss: 0.4133\n",
      "Epoch [337/2000], Loss: 0.4131\n",
      "Epoch [338/2000], Loss: 0.4123\n",
      "Epoch [339/2000], Loss: 0.4229\n",
      "Epoch [340/2000], Loss: 0.4167\n",
      "Epoch [341/2000], Loss: 0.4116\n",
      "Epoch [342/2000], Loss: 0.4109\n",
      "Epoch [343/2000], Loss: 0.4171\n",
      "Epoch [344/2000], Loss: 0.4196\n",
      "Epoch [345/2000], Loss: 0.4148\n",
      "Epoch [346/2000], Loss: 0.4136\n",
      "Epoch [347/2000], Loss: 0.4110\n",
      "Epoch [348/2000], Loss: 0.4087\n",
      "Epoch [349/2000], Loss: 0.4082\n",
      "Epoch [350/2000], Loss: 0.4082\n",
      "Epoch [351/2000], Loss: 0.4092\n",
      "Epoch [352/2000], Loss: 0.4135\n",
      "Epoch [353/2000], Loss: 0.4285\n",
      "Epoch [354/2000], Loss: 0.4125\n",
      "Epoch [355/2000], Loss: 0.4117\n",
      "Epoch [356/2000], Loss: 0.4086\n",
      "Epoch [357/2000], Loss: 0.4087\n",
      "Epoch [358/2000], Loss: 0.4097\n",
      "Epoch [359/2000], Loss: 0.4258\n",
      "Epoch [360/2000], Loss: 0.4239\n",
      "Epoch [361/2000], Loss: 0.4158\n",
      "Epoch [362/2000], Loss: 0.4160\n",
      "Epoch [363/2000], Loss: 0.4091\n",
      "Epoch [364/2000], Loss: 0.4059\n",
      "Epoch [365/2000], Loss: 0.4072\n",
      "Epoch [366/2000], Loss: 0.4086\n",
      "Epoch [367/2000], Loss: 0.4079\n",
      "Epoch [368/2000], Loss: 0.4071\n",
      "Epoch [369/2000], Loss: 0.4081\n",
      "Epoch [370/2000], Loss: 0.4083\n",
      "Epoch [371/2000], Loss: 0.4091\n",
      "Epoch [372/2000], Loss: 0.4061\n",
      "Epoch [373/2000], Loss: 0.4057\n",
      "Epoch [374/2000], Loss: 0.4049\n",
      "Epoch [375/2000], Loss: 0.4063\n",
      "Epoch [376/2000], Loss: 0.4041\n",
      "Epoch [377/2000], Loss: 0.4040\n",
      "Epoch [378/2000], Loss: 0.4037\n",
      "Epoch [379/2000], Loss: 0.4049\n",
      "Epoch [380/2000], Loss: 0.4070\n",
      "Epoch [381/2000], Loss: 0.4127\n",
      "Epoch [382/2000], Loss: 0.4155\n",
      "Epoch [383/2000], Loss: 0.4039\n",
      "Epoch [384/2000], Loss: 0.4037\n",
      "Epoch [385/2000], Loss: 0.4049\n",
      "Epoch [386/2000], Loss: 0.4049\n",
      "Epoch [387/2000], Loss: 0.4090\n",
      "Epoch [388/2000], Loss: 0.4052\n",
      "Epoch [389/2000], Loss: 0.4089\n",
      "Epoch [390/2000], Loss: 0.4075\n",
      "Epoch [391/2000], Loss: 0.4112\n",
      "Epoch [392/2000], Loss: 0.4123\n",
      "Epoch [393/2000], Loss: 0.4070\n",
      "Epoch [394/2000], Loss: 0.4132\n",
      "Epoch [395/2000], Loss: 0.4089\n",
      "Epoch [396/2000], Loss: 0.4076\n",
      "Epoch [397/2000], Loss: 0.4052\n",
      "Epoch [398/2000], Loss: 0.4095\n",
      "Epoch [399/2000], Loss: 0.4049\n",
      "Epoch [400/2000], Loss: 0.4026\n",
      "Epoch [401/2000], Loss: 0.4025\n",
      "Epoch [402/2000], Loss: 0.4018\n",
      "Epoch [403/2000], Loss: 0.4018\n",
      "Epoch [404/2000], Loss: 0.4049\n",
      "Epoch [405/2000], Loss: 0.4025\n",
      "Epoch [406/2000], Loss: 0.4030\n",
      "Epoch [407/2000], Loss: 0.4101\n",
      "Epoch [408/2000], Loss: 0.4060\n",
      "Epoch [409/2000], Loss: 0.4033\n",
      "Epoch [410/2000], Loss: 0.4042\n",
      "Epoch [411/2000], Loss: 0.4054\n",
      "Epoch [412/2000], Loss: 0.4065\n",
      "Epoch [413/2000], Loss: 0.4176\n",
      "Epoch [414/2000], Loss: 0.4291\n",
      "Epoch [415/2000], Loss: 0.4244\n",
      "Epoch [416/2000], Loss: 0.4179\n",
      "Epoch [417/2000], Loss: 0.4086\n",
      "Epoch [418/2000], Loss: 0.4061\n",
      "Epoch [419/2000], Loss: 0.4061\n",
      "Epoch [420/2000], Loss: 0.4051\n",
      "Epoch [421/2000], Loss: 0.4038\n",
      "Epoch [422/2000], Loss: 0.4089\n",
      "Epoch [423/2000], Loss: 0.4088\n",
      "Epoch [424/2000], Loss: 0.4057\n",
      "Epoch [425/2000], Loss: 0.4033\n",
      "Epoch [426/2000], Loss: 0.4018\n",
      "Epoch [427/2000], Loss: 0.4037\n",
      "Epoch [428/2000], Loss: 0.4061\n",
      "Epoch [429/2000], Loss: 0.4056\n",
      "Epoch [430/2000], Loss: 0.4054\n",
      "Epoch [431/2000], Loss: 0.4042\n",
      "Epoch [432/2000], Loss: 0.4056\n",
      "Epoch [433/2000], Loss: 0.4003\n",
      "Epoch [434/2000], Loss: 0.4005\n",
      "Epoch [435/2000], Loss: 0.4008\n",
      "Epoch [436/2000], Loss: 0.4017\n",
      "Epoch [437/2000], Loss: 0.3990\n",
      "Epoch [438/2000], Loss: 0.3980\n",
      "Epoch [439/2000], Loss: 0.3978\n",
      "Epoch [440/2000], Loss: 0.3993\n",
      "Epoch [441/2000], Loss: 0.4006\n",
      "Epoch [442/2000], Loss: 0.3995\n",
      "Epoch [443/2000], Loss: 0.4003\n",
      "Epoch [444/2000], Loss: 0.4027\n",
      "Epoch [445/2000], Loss: 0.4007\n",
      "Epoch [446/2000], Loss: 0.4038\n",
      "Epoch [447/2000], Loss: 0.4018\n",
      "Epoch [448/2000], Loss: 0.4003\n",
      "Epoch [449/2000], Loss: 0.4019\n",
      "Epoch [450/2000], Loss: 0.3970\n",
      "Epoch [451/2000], Loss: 0.4049\n",
      "Epoch [452/2000], Loss: 0.4070\n",
      "Epoch [453/2000], Loss: 0.4061\n",
      "Epoch [454/2000], Loss: 0.4057\n",
      "Epoch [455/2000], Loss: 0.4190\n",
      "Epoch [456/2000], Loss: 0.4048\n",
      "Epoch [457/2000], Loss: 0.4000\n",
      "Epoch [458/2000], Loss: 0.3990\n",
      "Epoch [459/2000], Loss: 0.4161\n",
      "Epoch [460/2000], Loss: 0.4025\n",
      "Epoch [461/2000], Loss: 0.4070\n",
      "Epoch [462/2000], Loss: 0.4019\n",
      "Epoch [463/2000], Loss: 0.3980\n",
      "Epoch [464/2000], Loss: 0.4024\n",
      "Epoch [465/2000], Loss: 0.4025\n",
      "Epoch [466/2000], Loss: 0.4001\n",
      "Epoch [467/2000], Loss: 0.3979\n",
      "Epoch [468/2000], Loss: 0.3967\n",
      "Epoch [469/2000], Loss: 0.3975\n",
      "Epoch [470/2000], Loss: 0.3974\n",
      "Epoch [471/2000], Loss: 0.3999\n",
      "Epoch [472/2000], Loss: 0.3965\n",
      "Epoch [473/2000], Loss: 0.3982\n",
      "Epoch [474/2000], Loss: 0.4044\n",
      "Epoch [475/2000], Loss: 0.4146\n",
      "Epoch [476/2000], Loss: 0.4209\n",
      "Epoch [477/2000], Loss: 0.4000\n",
      "Epoch [478/2000], Loss: 0.4055\n",
      "Epoch [479/2000], Loss: 0.4044\n",
      "Epoch [480/2000], Loss: 0.4045\n",
      "Epoch [481/2000], Loss: 0.3998\n",
      "Epoch [482/2000], Loss: 0.3968\n",
      "Epoch [483/2000], Loss: 0.3992\n",
      "Epoch [484/2000], Loss: 0.3964\n",
      "Epoch [485/2000], Loss: 0.3953\n",
      "Epoch [486/2000], Loss: 0.4002\n",
      "Epoch [487/2000], Loss: 0.3954\n",
      "Epoch [488/2000], Loss: 0.3988\n",
      "Epoch [489/2000], Loss: 0.3969\n",
      "Epoch [490/2000], Loss: 0.3951\n",
      "Epoch [491/2000], Loss: 0.3970\n",
      "Epoch [492/2000], Loss: 0.3960\n",
      "Epoch [493/2000], Loss: 0.3971\n",
      "Epoch [494/2000], Loss: 0.3948\n",
      "Epoch [495/2000], Loss: 0.4094\n",
      "Epoch [496/2000], Loss: 0.4007\n",
      "Epoch [497/2000], Loss: 0.4286\n",
      "Epoch [498/2000], Loss: 0.4147\n",
      "Epoch [499/2000], Loss: 0.3989\n",
      "Epoch [500/2000], Loss: 0.3960\n",
      "Epoch [501/2000], Loss: 0.3939\n",
      "Epoch [502/2000], Loss: 0.3981\n",
      "Epoch [503/2000], Loss: 0.3980\n",
      "Epoch [504/2000], Loss: 0.4055\n",
      "Epoch [505/2000], Loss: 0.4037\n",
      "Epoch [506/2000], Loss: 0.3958\n",
      "Epoch [507/2000], Loss: 0.3980\n",
      "Epoch [508/2000], Loss: 0.3954\n",
      "Epoch [509/2000], Loss: 0.3958\n",
      "Epoch [510/2000], Loss: 0.3935\n",
      "Epoch [511/2000], Loss: 0.4091\n",
      "Epoch [512/2000], Loss: 0.4824\n",
      "Epoch [513/2000], Loss: 0.4236\n",
      "Epoch [514/2000], Loss: 0.4018\n",
      "Epoch [515/2000], Loss: 0.3977\n",
      "Epoch [516/2000], Loss: 0.3956\n",
      "Epoch [517/2000], Loss: 0.3940\n",
      "Epoch [518/2000], Loss: 0.3936\n",
      "Epoch [519/2000], Loss: 0.3963\n",
      "Epoch [520/2000], Loss: 0.3945\n",
      "Epoch [521/2000], Loss: 0.3940\n",
      "Epoch [522/2000], Loss: 0.3949\n",
      "Epoch [523/2000], Loss: 0.3973\n",
      "Epoch [524/2000], Loss: 0.3950\n",
      "Epoch [525/2000], Loss: 0.3918\n",
      "Epoch [526/2000], Loss: 0.3936\n",
      "Epoch [527/2000], Loss: 0.4125\n",
      "Epoch [528/2000], Loss: 0.3989\n",
      "Epoch [529/2000], Loss: 0.3988\n",
      "Epoch [530/2000], Loss: 0.3951\n",
      "Epoch [531/2000], Loss: 0.3934\n",
      "Epoch [532/2000], Loss: 0.3918\n",
      "Epoch [533/2000], Loss: 0.3914\n",
      "Epoch [534/2000], Loss: 0.3929\n",
      "Epoch [535/2000], Loss: 0.3909\n",
      "Epoch [536/2000], Loss: 0.4089\n",
      "Epoch [537/2000], Loss: 0.4002\n",
      "Epoch [538/2000], Loss: 0.4010\n",
      "Epoch [539/2000], Loss: 0.4086\n",
      "Epoch [540/2000], Loss: 0.4085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [541/2000], Loss: 0.4087\n",
      "Epoch [542/2000], Loss: 0.4209\n",
      "Epoch [543/2000], Loss: 0.3970\n",
      "Epoch [544/2000], Loss: 0.3973\n",
      "Epoch [545/2000], Loss: 0.3931\n",
      "Epoch [546/2000], Loss: 0.3914\n",
      "Epoch [547/2000], Loss: 0.3918\n",
      "Epoch [548/2000], Loss: 0.3919\n",
      "Epoch [549/2000], Loss: 0.3909\n",
      "Epoch [550/2000], Loss: 0.3903\n",
      "Epoch [551/2000], Loss: 0.3942\n",
      "Epoch [552/2000], Loss: 0.3921\n",
      "Epoch [553/2000], Loss: 0.3904\n",
      "Epoch [554/2000], Loss: 0.4029\n",
      "Epoch [555/2000], Loss: 0.4132\n",
      "Epoch [556/2000], Loss: 0.3989\n",
      "Epoch [557/2000], Loss: 0.4312\n",
      "Epoch [558/2000], Loss: 0.4132\n",
      "Epoch [559/2000], Loss: 0.4136\n",
      "Epoch [560/2000], Loss: 0.3967\n",
      "Epoch [561/2000], Loss: 0.3926\n",
      "Epoch [562/2000], Loss: 0.3927\n",
      "Epoch [563/2000], Loss: 0.3912\n",
      "Epoch [564/2000], Loss: 0.3892\n",
      "Epoch [565/2000], Loss: 0.3898\n",
      "Epoch [566/2000], Loss: 0.3882\n",
      "Epoch [567/2000], Loss: 0.3936\n",
      "Epoch [568/2000], Loss: 0.3920\n",
      "Epoch [569/2000], Loss: 0.3925\n",
      "Epoch [570/2000], Loss: 0.3964\n",
      "Epoch [571/2000], Loss: 0.3899\n",
      "Epoch [572/2000], Loss: 0.3905\n",
      "Epoch [573/2000], Loss: 0.4228\n",
      "Epoch [574/2000], Loss: 0.4097\n",
      "Epoch [575/2000], Loss: 0.4025\n",
      "Epoch [576/2000], Loss: 0.4238\n",
      "Epoch [577/2000], Loss: 0.3955\n",
      "Epoch [578/2000], Loss: 0.4270\n",
      "Epoch [579/2000], Loss: 0.4072\n",
      "Epoch [580/2000], Loss: 0.4087\n",
      "Epoch [581/2000], Loss: 0.3908\n",
      "Epoch [582/2000], Loss: 0.3889\n",
      "Epoch [583/2000], Loss: 0.3887\n",
      "Epoch [584/2000], Loss: 0.3911\n",
      "Epoch [585/2000], Loss: 0.3883\n",
      "Epoch [586/2000], Loss: 0.3873\n",
      "Epoch [587/2000], Loss: 0.4008\n",
      "Epoch [588/2000], Loss: 0.3941\n",
      "Epoch [589/2000], Loss: 0.3905\n",
      "Epoch [590/2000], Loss: 0.3879\n",
      "Epoch [591/2000], Loss: 0.3905\n",
      "Epoch [592/2000], Loss: 0.4105\n",
      "Epoch [593/2000], Loss: 0.4017\n",
      "Epoch [594/2000], Loss: 0.3918\n",
      "Epoch [595/2000], Loss: 0.3968\n",
      "Epoch [596/2000], Loss: 0.3960\n",
      "Epoch [597/2000], Loss: 0.3958\n",
      "Epoch [598/2000], Loss: 0.3893\n",
      "Epoch [599/2000], Loss: 0.3937\n",
      "Epoch [600/2000], Loss: 0.3970\n",
      "Epoch [601/2000], Loss: 0.3967\n",
      "Epoch [602/2000], Loss: 0.4038\n",
      "Epoch [603/2000], Loss: 0.4142\n",
      "Epoch [604/2000], Loss: 0.3879\n",
      "Epoch [605/2000], Loss: 0.3899\n",
      "Epoch [606/2000], Loss: 0.3912\n",
      "Epoch [607/2000], Loss: 0.3876\n",
      "Epoch [608/2000], Loss: 0.3866\n",
      "Epoch [609/2000], Loss: 0.3853\n",
      "Epoch [610/2000], Loss: 0.3864\n",
      "Epoch [611/2000], Loss: 0.3861\n",
      "Epoch [612/2000], Loss: 0.3854\n",
      "Epoch [613/2000], Loss: 0.3855\n",
      "Epoch [614/2000], Loss: 0.3879\n",
      "Epoch [615/2000], Loss: 0.4024\n",
      "Epoch [616/2000], Loss: 0.4182\n",
      "Epoch [617/2000], Loss: 0.4318\n",
      "Epoch [618/2000], Loss: 0.3976\n",
      "Epoch [619/2000], Loss: 0.3914\n",
      "Epoch [620/2000], Loss: 0.4057\n",
      "Epoch [621/2000], Loss: 0.3951\n",
      "Epoch [622/2000], Loss: 0.3857\n",
      "Epoch [623/2000], Loss: 0.3852\n",
      "Epoch [624/2000], Loss: 0.3841\n",
      "Epoch [625/2000], Loss: 0.3854\n",
      "Epoch [626/2000], Loss: 0.3877\n",
      "Epoch [627/2000], Loss: 0.3899\n",
      "Epoch [628/2000], Loss: 0.4036\n",
      "Epoch [629/2000], Loss: 0.3888\n",
      "Epoch [630/2000], Loss: 0.3859\n",
      "Epoch [631/2000], Loss: 0.3854\n",
      "Epoch [632/2000], Loss: 0.3885\n",
      "Epoch [633/2000], Loss: 0.3848\n",
      "Epoch [634/2000], Loss: 0.3847\n",
      "Epoch [635/2000], Loss: 0.3928\n",
      "Epoch [636/2000], Loss: 0.4163\n",
      "Epoch [637/2000], Loss: 0.4064\n",
      "Epoch [638/2000], Loss: 0.4360\n",
      "Epoch [639/2000], Loss: 0.3937\n",
      "Epoch [640/2000], Loss: 0.3915\n",
      "Epoch [641/2000], Loss: 0.3873\n",
      "Epoch [642/2000], Loss: 0.3870\n",
      "Epoch [643/2000], Loss: 0.3876\n",
      "Epoch [644/2000], Loss: 0.3891\n",
      "Epoch [645/2000], Loss: 0.3915\n",
      "Epoch [646/2000], Loss: 0.3980\n",
      "Epoch [647/2000], Loss: 0.4085\n",
      "Epoch [648/2000], Loss: 0.4268\n",
      "Epoch [649/2000], Loss: 0.4054\n",
      "Epoch [650/2000], Loss: 0.3975\n",
      "Epoch [651/2000], Loss: 0.3844\n",
      "Epoch [652/2000], Loss: 0.3838\n",
      "Epoch [653/2000], Loss: 0.3841\n",
      "Epoch [654/2000], Loss: 0.3818\n",
      "Epoch [655/2000], Loss: 0.3830\n",
      "Epoch [656/2000], Loss: 0.3797\n",
      "Epoch [657/2000], Loss: 0.3818\n",
      "Epoch [658/2000], Loss: 0.3794\n",
      "Epoch [659/2000], Loss: 0.3831\n",
      "Epoch [660/2000], Loss: 0.3920\n",
      "Epoch [661/2000], Loss: 0.5014\n",
      "Epoch [662/2000], Loss: 0.4492\n",
      "Epoch [663/2000], Loss: 0.4272\n",
      "Epoch [664/2000], Loss: 0.4301\n",
      "Epoch [665/2000], Loss: 0.3945\n",
      "Epoch [666/2000], Loss: 0.3855\n",
      "Epoch [667/2000], Loss: 0.3830\n",
      "Epoch [668/2000], Loss: 0.3811\n",
      "Epoch [669/2000], Loss: 0.3824\n",
      "Epoch [670/2000], Loss: 0.3806\n",
      "Epoch [671/2000], Loss: 0.3816\n",
      "Epoch [672/2000], Loss: 0.3842\n",
      "Epoch [673/2000], Loss: 0.3824\n",
      "Epoch [674/2000], Loss: 0.3909\n",
      "Epoch [675/2000], Loss: 0.3989\n",
      "Epoch [676/2000], Loss: 0.3925\n",
      "Epoch [677/2000], Loss: 0.3860\n",
      "Epoch [678/2000], Loss: 0.3871\n",
      "Epoch [679/2000], Loss: 0.3831\n",
      "Epoch [680/2000], Loss: 0.3898\n",
      "Epoch [681/2000], Loss: 0.3906\n",
      "Epoch [682/2000], Loss: 0.3853\n",
      "Epoch [683/2000], Loss: 0.3925\n",
      "Epoch [684/2000], Loss: 0.3987\n",
      "Epoch [685/2000], Loss: 0.3955\n",
      "Epoch [686/2000], Loss: 0.3942\n",
      "Epoch [687/2000], Loss: 0.3859\n",
      "Epoch [688/2000], Loss: 0.3845\n",
      "Epoch [689/2000], Loss: 0.3829\n",
      "Epoch [690/2000], Loss: 0.3821\n",
      "Epoch [691/2000], Loss: 0.3824\n",
      "Epoch [692/2000], Loss: 0.3907\n",
      "Epoch [693/2000], Loss: 0.3829\n",
      "Epoch [694/2000], Loss: 0.4253\n",
      "Epoch [695/2000], Loss: 0.4041\n",
      "Epoch [696/2000], Loss: 0.4169\n",
      "Epoch [697/2000], Loss: 0.3846\n",
      "Epoch [698/2000], Loss: 0.3887\n",
      "Epoch [699/2000], Loss: 0.3800\n",
      "Epoch [700/2000], Loss: 0.3807\n",
      "Epoch [701/2000], Loss: 0.3800\n",
      "Epoch [702/2000], Loss: 0.3790\n",
      "Epoch [703/2000], Loss: 0.3835\n",
      "Epoch [704/2000], Loss: 0.3817\n",
      "Epoch [705/2000], Loss: 0.3794\n",
      "Epoch [706/2000], Loss: 0.3790\n",
      "Epoch [707/2000], Loss: 0.3821\n",
      "Epoch [708/2000], Loss: 0.3888\n",
      "Epoch [709/2000], Loss: 0.3830\n",
      "Epoch [710/2000], Loss: 0.3859\n",
      "Epoch [711/2000], Loss: 0.4145\n",
      "Epoch [712/2000], Loss: 0.3873\n",
      "Epoch [713/2000], Loss: 0.4102\n",
      "Epoch [714/2000], Loss: 0.4058\n",
      "Epoch [715/2000], Loss: 0.4212\n",
      "Epoch [716/2000], Loss: 0.3833\n",
      "Epoch [717/2000], Loss: 0.3811\n",
      "Epoch [718/2000], Loss: 0.3776\n",
      "Epoch [719/2000], Loss: 0.3772\n",
      "Epoch [720/2000], Loss: 0.3758\n",
      "Epoch [721/2000], Loss: 0.3747\n",
      "Epoch [722/2000], Loss: 0.3788\n",
      "Epoch [723/2000], Loss: 0.3778\n",
      "Epoch [724/2000], Loss: 0.3772\n",
      "Epoch [725/2000], Loss: 0.3744\n",
      "Epoch [726/2000], Loss: 0.3790\n",
      "Epoch [727/2000], Loss: 0.3800\n",
      "Epoch [728/2000], Loss: 0.3801\n",
      "Epoch [729/2000], Loss: 0.3738\n",
      "Epoch [730/2000], Loss: 0.3729\n",
      "Epoch [731/2000], Loss: 0.4029\n",
      "Epoch [732/2000], Loss: 0.4142\n",
      "Epoch [733/2000], Loss: 0.4401\n",
      "Epoch [734/2000], Loss: 0.4084\n",
      "Epoch [735/2000], Loss: 0.3859\n",
      "Epoch [736/2000], Loss: 0.3813\n",
      "Epoch [737/2000], Loss: 0.3781\n",
      "Epoch [738/2000], Loss: 0.3774\n",
      "Epoch [739/2000], Loss: 0.3758\n",
      "Epoch [740/2000], Loss: 0.3838\n",
      "Epoch [741/2000], Loss: 0.3772\n",
      "Epoch [742/2000], Loss: 0.3818\n",
      "Epoch [743/2000], Loss: 0.3894\n",
      "Epoch [744/2000], Loss: 0.3877\n",
      "Epoch [745/2000], Loss: 0.3846\n",
      "Epoch [746/2000], Loss: 0.4007\n",
      "Epoch [747/2000], Loss: 0.3888\n",
      "Epoch [748/2000], Loss: 0.3800\n",
      "Epoch [749/2000], Loss: 0.3761\n",
      "Epoch [750/2000], Loss: 0.3770\n",
      "Epoch [751/2000], Loss: 0.3778\n",
      "Epoch [752/2000], Loss: 0.3921\n",
      "Epoch [753/2000], Loss: 0.3833\n",
      "Epoch [754/2000], Loss: 0.3791\n",
      "Epoch [755/2000], Loss: 0.4509\n",
      "Epoch [756/2000], Loss: 0.3966\n",
      "Epoch [757/2000], Loss: 0.3816\n",
      "Epoch [758/2000], Loss: 0.3788\n",
      "Epoch [759/2000], Loss: 0.3748\n",
      "Epoch [760/2000], Loss: 0.3764\n",
      "Epoch [761/2000], Loss: 0.3745\n",
      "Epoch [762/2000], Loss: 0.3739\n",
      "Epoch [763/2000], Loss: 0.3739\n",
      "Epoch [764/2000], Loss: 0.3747\n",
      "Epoch [765/2000], Loss: 0.3728\n",
      "Epoch [766/2000], Loss: 0.3741\n",
      "Epoch [767/2000], Loss: 0.3737\n",
      "Epoch [768/2000], Loss: 0.3737\n",
      "Epoch [769/2000], Loss: 0.3742\n",
      "Epoch [770/2000], Loss: 0.3852\n",
      "Epoch [771/2000], Loss: 0.4062\n",
      "Epoch [772/2000], Loss: 0.4002\n",
      "Epoch [773/2000], Loss: 0.3885\n",
      "Epoch [774/2000], Loss: 0.3832\n",
      "Epoch [775/2000], Loss: 0.3738\n",
      "Epoch [776/2000], Loss: 0.3823\n",
      "Epoch [777/2000], Loss: 0.3841\n",
      "Epoch [778/2000], Loss: 0.3824\n",
      "Epoch [779/2000], Loss: 0.3770\n",
      "Epoch [780/2000], Loss: 0.3767\n",
      "Epoch [781/2000], Loss: 0.3743\n",
      "Epoch [782/2000], Loss: 0.3833\n",
      "Epoch [783/2000], Loss: 0.3820\n",
      "Epoch [784/2000], Loss: 0.3739\n",
      "Epoch [785/2000], Loss: 0.3754\n",
      "Epoch [786/2000], Loss: 0.3733\n",
      "Epoch [787/2000], Loss: 0.3750\n",
      "Epoch [788/2000], Loss: 0.3754\n",
      "Epoch [789/2000], Loss: 0.3794\n",
      "Epoch [790/2000], Loss: 0.4008\n",
      "Epoch [791/2000], Loss: 0.4023\n",
      "Epoch [792/2000], Loss: 0.3888\n",
      "Epoch [793/2000], Loss: 0.3761\n",
      "Epoch [794/2000], Loss: 0.3757\n",
      "Epoch [795/2000], Loss: 0.3755\n",
      "Epoch [796/2000], Loss: 0.3807\n",
      "Epoch [797/2000], Loss: 0.3813\n",
      "Epoch [798/2000], Loss: 0.3836\n",
      "Epoch [799/2000], Loss: 0.3838\n",
      "Epoch [800/2000], Loss: 0.3744\n",
      "Epoch [801/2000], Loss: 0.3853\n",
      "Epoch [802/2000], Loss: 0.3823\n",
      "Epoch [803/2000], Loss: 0.3781\n",
      "Epoch [804/2000], Loss: 0.3749\n",
      "Epoch [805/2000], Loss: 0.3722\n",
      "Epoch [806/2000], Loss: 0.3718\n",
      "Epoch [807/2000], Loss: 0.3693\n",
      "Epoch [808/2000], Loss: 0.3730\n",
      "Epoch [809/2000], Loss: 0.3782\n",
      "Epoch [810/2000], Loss: 0.3898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [811/2000], Loss: 0.3780\n",
      "Epoch [812/2000], Loss: 0.3707\n",
      "Epoch [813/2000], Loss: 0.3732\n",
      "Epoch [814/2000], Loss: 0.3707\n",
      "Epoch [815/2000], Loss: 0.3707\n",
      "Epoch [816/2000], Loss: 0.3828\n",
      "Epoch [817/2000], Loss: 0.4830\n",
      "Epoch [818/2000], Loss: 0.4356\n",
      "Epoch [819/2000], Loss: 0.3947\n",
      "Epoch [820/2000], Loss: 0.3964\n",
      "Epoch [821/2000], Loss: 0.3794\n",
      "Epoch [822/2000], Loss: 0.3748\n",
      "Epoch [823/2000], Loss: 0.3759\n",
      "Epoch [824/2000], Loss: 0.3733\n",
      "Epoch [825/2000], Loss: 0.3700\n",
      "Epoch [826/2000], Loss: 0.3693\n",
      "Epoch [827/2000], Loss: 0.3690\n",
      "Epoch [828/2000], Loss: 0.3672\n",
      "Epoch [829/2000], Loss: 0.3670\n",
      "Epoch [830/2000], Loss: 0.3662\n",
      "Epoch [831/2000], Loss: 0.3679\n",
      "Epoch [832/2000], Loss: 0.3780\n",
      "Epoch [833/2000], Loss: 0.3694\n",
      "Epoch [834/2000], Loss: 0.3755\n",
      "Epoch [835/2000], Loss: 0.3864\n",
      "Epoch [836/2000], Loss: 0.3792\n",
      "Epoch [837/2000], Loss: 0.4318\n",
      "Epoch [838/2000], Loss: 0.3935\n",
      "Epoch [839/2000], Loss: 0.3816\n",
      "Epoch [840/2000], Loss: 0.3771\n",
      "Epoch [841/2000], Loss: 0.3709\n",
      "Epoch [842/2000], Loss: 0.3676\n",
      "Epoch [843/2000], Loss: 0.3666\n",
      "Epoch [844/2000], Loss: 0.3666\n",
      "Epoch [845/2000], Loss: 0.3690\n",
      "Epoch [846/2000], Loss: 0.3704\n",
      "Epoch [847/2000], Loss: 0.3731\n",
      "Epoch [848/2000], Loss: 0.3825\n",
      "Epoch [849/2000], Loss: 0.3829\n",
      "Epoch [850/2000], Loss: 0.4020\n",
      "Epoch [851/2000], Loss: 0.4162\n",
      "Epoch [852/2000], Loss: 0.3893\n",
      "Epoch [853/2000], Loss: 0.3758\n",
      "Epoch [854/2000], Loss: 0.3720\n",
      "Epoch [855/2000], Loss: 0.3716\n",
      "Epoch [856/2000], Loss: 0.3697\n",
      "Epoch [857/2000], Loss: 0.3681\n",
      "Epoch [858/2000], Loss: 0.3675\n",
      "Epoch [859/2000], Loss: 0.3667\n",
      "Epoch [860/2000], Loss: 0.3664\n",
      "Epoch [861/2000], Loss: 0.3665\n",
      "Epoch [862/2000], Loss: 0.3696\n",
      "Epoch [863/2000], Loss: 0.3720\n",
      "Epoch [864/2000], Loss: 0.3715\n",
      "Epoch [865/2000], Loss: 0.3717\n",
      "Epoch [866/2000], Loss: 0.4002\n",
      "Epoch [867/2000], Loss: 0.4050\n",
      "Epoch [868/2000], Loss: 0.3983\n",
      "Epoch [869/2000], Loss: 0.3712\n",
      "Epoch [870/2000], Loss: 0.3693\n",
      "Epoch [871/2000], Loss: 0.3666\n",
      "Epoch [872/2000], Loss: 0.3653\n",
      "Epoch [873/2000], Loss: 0.3665\n",
      "Epoch [874/2000], Loss: 0.3660\n",
      "Epoch [875/2000], Loss: 0.3656\n",
      "Epoch [876/2000], Loss: 0.3736\n",
      "Epoch [877/2000], Loss: 0.3676\n",
      "Epoch [878/2000], Loss: 0.3755\n",
      "Epoch [879/2000], Loss: 0.3676\n",
      "Epoch [880/2000], Loss: 0.3690\n",
      "Epoch [881/2000], Loss: 0.3685\n",
      "Epoch [882/2000], Loss: 0.3725\n",
      "Epoch [883/2000], Loss: 0.3806\n",
      "Epoch [884/2000], Loss: 0.3926\n",
      "Epoch [885/2000], Loss: 0.4116\n",
      "Epoch [886/2000], Loss: 0.3744\n",
      "Epoch [887/2000], Loss: 0.3644\n",
      "Epoch [888/2000], Loss: 0.3643\n",
      "Epoch [889/2000], Loss: 0.3631\n",
      "Epoch [890/2000], Loss: 0.3637\n",
      "Epoch [891/2000], Loss: 0.3661\n",
      "Epoch [892/2000], Loss: 0.3722\n",
      "Epoch [893/2000], Loss: 0.3815\n",
      "Epoch [894/2000], Loss: 0.3691\n",
      "Epoch [895/2000], Loss: 0.3823\n",
      "Epoch [896/2000], Loss: 0.3729\n",
      "Epoch [897/2000], Loss: 0.3704\n",
      "Epoch [898/2000], Loss: 0.3686\n",
      "Epoch [899/2000], Loss: 0.3658\n",
      "Epoch [900/2000], Loss: 0.3753\n",
      "Epoch [901/2000], Loss: 0.3920\n",
      "Epoch [902/2000], Loss: 0.3948\n",
      "Epoch [903/2000], Loss: 0.3688\n",
      "Epoch [904/2000], Loss: 0.3660\n",
      "Epoch [905/2000], Loss: 0.3629\n",
      "Epoch [906/2000], Loss: 0.3637\n",
      "Epoch [907/2000], Loss: 0.3633\n",
      "Epoch [908/2000], Loss: 0.3646\n",
      "Epoch [909/2000], Loss: 0.3650\n",
      "Epoch [910/2000], Loss: 0.3639\n",
      "Epoch [911/2000], Loss: 0.3598\n",
      "Epoch [912/2000], Loss: 0.3666\n",
      "Epoch [913/2000], Loss: 0.3687\n",
      "Epoch [914/2000], Loss: 0.3711\n",
      "Epoch [915/2000], Loss: 0.3723\n",
      "Epoch [916/2000], Loss: 0.4139\n",
      "Epoch [917/2000], Loss: 0.3876\n",
      "Epoch [918/2000], Loss: 0.3657\n",
      "Epoch [919/2000], Loss: 0.3640\n",
      "Epoch [920/2000], Loss: 0.3654\n",
      "Epoch [921/2000], Loss: 0.3588\n",
      "Epoch [922/2000], Loss: 0.3607\n",
      "Epoch [923/2000], Loss: 0.3571\n",
      "Epoch [924/2000], Loss: 0.3603\n",
      "Epoch [925/2000], Loss: 0.3582\n",
      "Epoch [926/2000], Loss: 0.3549\n",
      "Epoch [927/2000], Loss: 0.3584\n",
      "Epoch [928/2000], Loss: 0.3611\n",
      "Epoch [929/2000], Loss: 0.3674\n",
      "Epoch [930/2000], Loss: 0.3626\n",
      "Epoch [931/2000], Loss: 0.3732\n",
      "Epoch [932/2000], Loss: 0.3931\n",
      "Epoch [933/2000], Loss: 0.5174\n",
      "Epoch [934/2000], Loss: 0.4128\n",
      "Epoch [935/2000], Loss: 0.3869\n",
      "Epoch [936/2000], Loss: 0.3756\n",
      "Epoch [937/2000], Loss: 0.3725\n",
      "Epoch [938/2000], Loss: 0.3712\n",
      "Epoch [939/2000], Loss: 0.3691\n",
      "Epoch [940/2000], Loss: 0.3673\n",
      "Epoch [941/2000], Loss: 0.3666\n",
      "Epoch [942/2000], Loss: 0.3671\n",
      "Epoch [943/2000], Loss: 0.3678\n",
      "Epoch [944/2000], Loss: 0.3658\n",
      "Epoch [945/2000], Loss: 0.3650\n",
      "Epoch [946/2000], Loss: 0.3698\n",
      "Epoch [947/2000], Loss: 0.3746\n",
      "Epoch [948/2000], Loss: 0.3753\n",
      "Epoch [949/2000], Loss: 0.3766\n",
      "Epoch [950/2000], Loss: 0.3712\n",
      "Epoch [951/2000], Loss: 0.3778\n",
      "Epoch [952/2000], Loss: 0.3914\n",
      "Epoch [953/2000], Loss: 0.3804\n",
      "Epoch [954/2000], Loss: 0.3724\n",
      "Epoch [955/2000], Loss: 0.3652\n",
      "Epoch [956/2000], Loss: 0.3634\n",
      "Epoch [957/2000], Loss: 0.3641\n",
      "Epoch [958/2000], Loss: 0.3660\n",
      "Epoch [959/2000], Loss: 0.3627\n",
      "Epoch [960/2000], Loss: 0.3629\n",
      "Epoch [961/2000], Loss: 0.3629\n",
      "Epoch [962/2000], Loss: 0.3696\n",
      "Epoch [963/2000], Loss: 0.3679\n",
      "Epoch [964/2000], Loss: 0.3709\n",
      "Epoch [965/2000], Loss: 0.3997\n",
      "Epoch [966/2000], Loss: 0.3852\n",
      "Epoch [967/2000], Loss: 0.3775\n",
      "Epoch [968/2000], Loss: 0.3700\n",
      "Epoch [969/2000], Loss: 0.3633\n",
      "Epoch [970/2000], Loss: 0.3573\n",
      "Epoch [971/2000], Loss: 0.3581\n",
      "Epoch [972/2000], Loss: 0.3635\n",
      "Epoch [973/2000], Loss: 0.3606\n",
      "Epoch [974/2000], Loss: 0.3583\n",
      "Epoch [975/2000], Loss: 0.3592\n",
      "Epoch [976/2000], Loss: 0.3588\n",
      "Epoch [977/2000], Loss: 0.3535\n",
      "Epoch [978/2000], Loss: 0.3641\n",
      "Epoch [979/2000], Loss: 0.3591\n",
      "Epoch [980/2000], Loss: 0.3652\n",
      "Epoch [981/2000], Loss: 0.3553\n",
      "Epoch [982/2000], Loss: 0.3827\n",
      "Epoch [983/2000], Loss: 0.3847\n",
      "Epoch [984/2000], Loss: 0.3715\n",
      "Epoch [985/2000], Loss: 0.3605\n",
      "Epoch [986/2000], Loss: 0.3587\n",
      "Epoch [987/2000], Loss: 0.3561\n",
      "Epoch [988/2000], Loss: 0.3534\n",
      "Epoch [989/2000], Loss: 0.3520\n",
      "Epoch [990/2000], Loss: 0.3564\n",
      "Epoch [991/2000], Loss: 0.3583\n",
      "Epoch [992/2000], Loss: 0.3560\n",
      "Epoch [993/2000], Loss: 0.3688\n",
      "Epoch [994/2000], Loss: 0.3783\n",
      "Epoch [995/2000], Loss: 0.3826\n",
      "Epoch [996/2000], Loss: 0.3979\n",
      "Epoch [997/2000], Loss: 0.3802\n",
      "Epoch [998/2000], Loss: 0.3625\n",
      "Epoch [999/2000], Loss: 0.3525\n",
      "Epoch [1000/2000], Loss: 0.3567\n",
      "Epoch [1001/2000], Loss: 0.3519\n",
      "Epoch [1002/2000], Loss: 0.3557\n",
      "Epoch [1003/2000], Loss: 0.3570\n",
      "Epoch [1004/2000], Loss: 0.3579\n",
      "Epoch [1005/2000], Loss: 0.3552\n",
      "Epoch [1006/2000], Loss: 0.3545\n",
      "Epoch [1007/2000], Loss: 0.3865\n",
      "Epoch [1008/2000], Loss: 0.3628\n",
      "Epoch [1009/2000], Loss: 0.3555\n",
      "Epoch [1010/2000], Loss: 0.3553\n",
      "Epoch [1011/2000], Loss: 0.3514\n",
      "Epoch [1012/2000], Loss: 0.3607\n",
      "Epoch [1013/2000], Loss: 0.3594\n",
      "Epoch [1014/2000], Loss: 0.3856\n",
      "Epoch [1015/2000], Loss: 0.3733\n",
      "Epoch [1016/2000], Loss: 0.3620\n",
      "Epoch [1017/2000], Loss: 0.3534\n",
      "Epoch [1018/2000], Loss: 0.3533\n",
      "Epoch [1019/2000], Loss: 0.3587\n",
      "Epoch [1020/2000], Loss: 0.3594\n",
      "Epoch [1021/2000], Loss: 0.3570\n",
      "Epoch [1022/2000], Loss: 0.3625\n",
      "Epoch [1023/2000], Loss: 0.3565\n",
      "Epoch [1024/2000], Loss: 0.3617\n",
      "Epoch [1025/2000], Loss: 0.3861\n",
      "Epoch [1026/2000], Loss: 0.3690\n",
      "Epoch [1027/2000], Loss: 0.3541\n",
      "Epoch [1028/2000], Loss: 0.3521\n",
      "Epoch [1029/2000], Loss: 0.3564\n",
      "Epoch [1030/2000], Loss: 0.3526\n",
      "Epoch [1031/2000], Loss: 0.3509\n",
      "Epoch [1032/2000], Loss: 0.3515\n",
      "Epoch [1033/2000], Loss: 0.3495\n",
      "Epoch [1034/2000], Loss: 0.3534\n",
      "Epoch [1035/2000], Loss: 0.3651\n",
      "Epoch [1036/2000], Loss: 0.3545\n",
      "Epoch [1037/2000], Loss: 0.4778\n",
      "Epoch [1038/2000], Loss: 0.3666\n",
      "Epoch [1039/2000], Loss: 0.3601\n",
      "Epoch [1040/2000], Loss: 0.3532\n",
      "Epoch [1041/2000], Loss: 0.3504\n",
      "Epoch [1042/2000], Loss: 0.3510\n",
      "Epoch [1043/2000], Loss: 0.3501\n",
      "Epoch [1044/2000], Loss: 0.3474\n",
      "Epoch [1045/2000], Loss: 0.3477\n",
      "Epoch [1046/2000], Loss: 0.3460\n",
      "Epoch [1047/2000], Loss: 0.3489\n",
      "Epoch [1048/2000], Loss: 0.3498\n",
      "Epoch [1049/2000], Loss: 0.3475\n",
      "Epoch [1050/2000], Loss: 0.3461\n",
      "Epoch [1051/2000], Loss: 0.3479\n",
      "Epoch [1052/2000], Loss: 0.3593\n",
      "Epoch [1053/2000], Loss: 0.4003\n",
      "Epoch [1054/2000], Loss: 0.3854\n",
      "Epoch [1055/2000], Loss: 0.3850\n",
      "Epoch [1056/2000], Loss: 0.3587\n",
      "Epoch [1057/2000], Loss: 0.3500\n",
      "Epoch [1058/2000], Loss: 0.3465\n",
      "Epoch [1059/2000], Loss: 0.3469\n",
      "Epoch [1060/2000], Loss: 0.3450\n",
      "Epoch [1061/2000], Loss: 0.3478\n",
      "Epoch [1062/2000], Loss: 0.3449\n",
      "Epoch [1063/2000], Loss: 0.3499\n",
      "Epoch [1064/2000], Loss: 0.3481\n",
      "Epoch [1065/2000], Loss: 0.3505\n",
      "Epoch [1066/2000], Loss: 0.3535\n",
      "Epoch [1067/2000], Loss: 0.3487\n",
      "Epoch [1068/2000], Loss: 0.3614\n",
      "Epoch [1069/2000], Loss: 0.3721\n",
      "Epoch [1070/2000], Loss: 0.3847\n",
      "Epoch [1071/2000], Loss: 0.3575\n",
      "Epoch [1072/2000], Loss: 0.3507\n",
      "Epoch [1073/2000], Loss: 0.3480\n",
      "Epoch [1074/2000], Loss: 0.3580\n",
      "Epoch [1075/2000], Loss: 0.3468\n",
      "Epoch [1076/2000], Loss: 0.3456\n",
      "Epoch [1077/2000], Loss: 0.3460\n",
      "Epoch [1078/2000], Loss: 0.3567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1079/2000], Loss: 0.3675\n",
      "Epoch [1080/2000], Loss: 0.3681\n",
      "Epoch [1081/2000], Loss: 0.3513\n",
      "Epoch [1082/2000], Loss: 0.3518\n",
      "Epoch [1083/2000], Loss: 0.3494\n",
      "Epoch [1084/2000], Loss: 0.3456\n",
      "Epoch [1085/2000], Loss: 0.3438\n",
      "Epoch [1086/2000], Loss: 0.3448\n",
      "Epoch [1087/2000], Loss: 0.3488\n",
      "Epoch [1088/2000], Loss: 0.3456\n",
      "Epoch [1089/2000], Loss: 0.3551\n",
      "Epoch [1090/2000], Loss: 0.3912\n",
      "Epoch [1091/2000], Loss: 0.3890\n",
      "Epoch [1092/2000], Loss: 0.3699\n",
      "Epoch [1093/2000], Loss: 0.3494\n",
      "Epoch [1094/2000], Loss: 0.3457\n",
      "Epoch [1095/2000], Loss: 0.3435\n",
      "Epoch [1096/2000], Loss: 0.3563\n",
      "Epoch [1097/2000], Loss: 0.3471\n",
      "Epoch [1098/2000], Loss: 0.3442\n",
      "Epoch [1099/2000], Loss: 0.3454\n",
      "Epoch [1100/2000], Loss: 0.3484\n",
      "Epoch [1101/2000], Loss: 0.3679\n",
      "Epoch [1102/2000], Loss: 0.3762\n",
      "Epoch [1103/2000], Loss: 0.3595\n",
      "Epoch [1104/2000], Loss: 0.3510\n",
      "Epoch [1105/2000], Loss: 0.3491\n",
      "Epoch [1106/2000], Loss: 0.3473\n",
      "Epoch [1107/2000], Loss: 0.3500\n",
      "Epoch [1108/2000], Loss: 0.3477\n",
      "Epoch [1109/2000], Loss: 0.3474\n",
      "Epoch [1110/2000], Loss: 0.3490\n",
      "Epoch [1111/2000], Loss: 0.3511\n",
      "Epoch [1112/2000], Loss: 0.3565\n",
      "Epoch [1113/2000], Loss: 0.3589\n",
      "Epoch [1114/2000], Loss: 0.3503\n",
      "Epoch [1115/2000], Loss: 0.3542\n",
      "Epoch [1116/2000], Loss: 0.4277\n",
      "Epoch [1117/2000], Loss: 0.3641\n",
      "Epoch [1118/2000], Loss: 0.3522\n",
      "Epoch [1119/2000], Loss: 0.3453\n",
      "Epoch [1120/2000], Loss: 0.3455\n",
      "Epoch [1121/2000], Loss: 0.3462\n",
      "Epoch [1122/2000], Loss: 0.3464\n",
      "Epoch [1123/2000], Loss: 0.3487\n",
      "Epoch [1124/2000], Loss: 0.3675\n",
      "Epoch [1125/2000], Loss: 0.3666\n",
      "Epoch [1126/2000], Loss: 0.3558\n",
      "Epoch [1127/2000], Loss: 0.3444\n",
      "Epoch [1128/2000], Loss: 0.3469\n",
      "Epoch [1129/2000], Loss: 0.3467\n",
      "Epoch [1130/2000], Loss: 0.3429\n",
      "Epoch [1131/2000], Loss: 0.3454\n",
      "Epoch [1132/2000], Loss: 0.3427\n",
      "Epoch [1133/2000], Loss: 0.3471\n",
      "Epoch [1134/2000], Loss: 0.3549\n",
      "Epoch [1135/2000], Loss: 0.3628\n",
      "Epoch [1136/2000], Loss: 0.3652\n",
      "Epoch [1137/2000], Loss: 0.3915\n",
      "Epoch [1138/2000], Loss: 0.3486\n",
      "Epoch [1139/2000], Loss: 0.3549\n",
      "Epoch [1140/2000], Loss: 0.3471\n",
      "Epoch [1141/2000], Loss: 0.3433\n",
      "Epoch [1142/2000], Loss: 0.3458\n",
      "Epoch [1143/2000], Loss: 0.3420\n",
      "Epoch [1144/2000], Loss: 0.3419\n",
      "Epoch [1145/2000], Loss: 0.3614\n",
      "Epoch [1146/2000], Loss: 0.3537\n",
      "Epoch [1147/2000], Loss: 0.3438\n",
      "Epoch [1148/2000], Loss: 0.3428\n",
      "Epoch [1149/2000], Loss: 0.3432\n",
      "Epoch [1150/2000], Loss: 0.3522\n",
      "Epoch [1151/2000], Loss: 0.3878\n",
      "Epoch [1152/2000], Loss: 0.3701\n",
      "Epoch [1153/2000], Loss: 0.3667\n",
      "Epoch [1154/2000], Loss: 0.3598\n",
      "Epoch [1155/2000], Loss: 0.3486\n",
      "Epoch [1156/2000], Loss: 0.3442\n",
      "Epoch [1157/2000], Loss: 0.3416\n",
      "Epoch [1158/2000], Loss: 0.3451\n",
      "Epoch [1159/2000], Loss: 0.3421\n",
      "Epoch [1160/2000], Loss: 0.3477\n",
      "Epoch [1161/2000], Loss: 0.3504\n",
      "Epoch [1162/2000], Loss: 0.3470\n",
      "Epoch [1163/2000], Loss: 0.3436\n",
      "Epoch [1164/2000], Loss: 0.3496\n",
      "Epoch [1165/2000], Loss: 0.3891\n",
      "Epoch [1166/2000], Loss: 0.3684\n",
      "Epoch [1167/2000], Loss: 0.3603\n",
      "Epoch [1168/2000], Loss: 0.3483\n",
      "Epoch [1169/2000], Loss: 0.3453\n",
      "Epoch [1170/2000], Loss: 0.3467\n",
      "Epoch [1171/2000], Loss: 0.3444\n",
      "Epoch [1172/2000], Loss: 0.3500\n",
      "Epoch [1173/2000], Loss: 0.3420\n",
      "Epoch [1174/2000], Loss: 0.3428\n",
      "Epoch [1175/2000], Loss: 0.3447\n",
      "Epoch [1176/2000], Loss: 0.3446\n",
      "Epoch [1177/2000], Loss: 0.3530\n",
      "Epoch [1178/2000], Loss: 0.3467\n",
      "Epoch [1179/2000], Loss: 0.3482\n",
      "Epoch [1180/2000], Loss: 0.3490\n",
      "Epoch [1181/2000], Loss: 0.3514\n",
      "Epoch [1182/2000], Loss: 0.3565\n",
      "Epoch [1183/2000], Loss: 0.3504\n",
      "Epoch [1184/2000], Loss: 0.3667\n",
      "Epoch [1185/2000], Loss: 0.3537\n",
      "Epoch [1186/2000], Loss: 0.3419\n",
      "Epoch [1187/2000], Loss: 0.3403\n",
      "Epoch [1188/2000], Loss: 0.3397\n",
      "Epoch [1189/2000], Loss: 0.3463\n",
      "Epoch [1190/2000], Loss: 0.3536\n",
      "Epoch [1191/2000], Loss: 0.3541\n",
      "Epoch [1192/2000], Loss: 0.3610\n",
      "Epoch [1193/2000], Loss: 0.4119\n",
      "Epoch [1194/2000], Loss: 0.3927\n",
      "Epoch [1195/2000], Loss: 0.3977\n",
      "Epoch [1196/2000], Loss: 0.3565\n",
      "Epoch [1197/2000], Loss: 0.3467\n",
      "Epoch [1198/2000], Loss: 0.3445\n",
      "Epoch [1199/2000], Loss: 0.3438\n",
      "Epoch [1200/2000], Loss: 0.3421\n",
      "Epoch [1201/2000], Loss: 0.3421\n",
      "Epoch [1202/2000], Loss: 0.3391\n",
      "Epoch [1203/2000], Loss: 0.3404\n",
      "Epoch [1204/2000], Loss: 0.3386\n",
      "Epoch [1205/2000], Loss: 0.3389\n",
      "Epoch [1206/2000], Loss: 0.3385\n",
      "Epoch [1207/2000], Loss: 0.3443\n",
      "Epoch [1208/2000], Loss: 0.3479\n",
      "Epoch [1209/2000], Loss: 0.3522\n",
      "Epoch [1210/2000], Loss: 0.3499\n",
      "Epoch [1211/2000], Loss: 0.3439\n",
      "Epoch [1212/2000], Loss: 0.3554\n",
      "Epoch [1213/2000], Loss: 0.3854\n",
      "Epoch [1214/2000], Loss: 0.3813\n",
      "Epoch [1215/2000], Loss: 0.4207\n",
      "Epoch [1216/2000], Loss: 0.3460\n",
      "Epoch [1217/2000], Loss: 0.3431\n",
      "Epoch [1218/2000], Loss: 0.3403\n",
      "Epoch [1219/2000], Loss: 0.3404\n",
      "Epoch [1220/2000], Loss: 0.3462\n",
      "Epoch [1221/2000], Loss: 0.3405\n",
      "Epoch [1222/2000], Loss: 0.3381\n",
      "Epoch [1223/2000], Loss: 0.3408\n",
      "Epoch [1224/2000], Loss: 0.3444\n",
      "Epoch [1225/2000], Loss: 0.3444\n",
      "Epoch [1226/2000], Loss: 0.3440\n",
      "Epoch [1227/2000], Loss: 0.3477\n",
      "Epoch [1228/2000], Loss: 0.3725\n",
      "Epoch [1229/2000], Loss: 0.3880\n",
      "Epoch [1230/2000], Loss: 0.3463\n",
      "Epoch [1231/2000], Loss: 0.3434\n",
      "Epoch [1232/2000], Loss: 0.3404\n",
      "Epoch [1233/2000], Loss: 0.3400\n",
      "Epoch [1234/2000], Loss: 0.3407\n",
      "Epoch [1235/2000], Loss: 0.3443\n",
      "Epoch [1236/2000], Loss: 0.3503\n",
      "Epoch [1237/2000], Loss: 0.3479\n",
      "Epoch [1238/2000], Loss: 0.3498\n",
      "Epoch [1239/2000], Loss: 0.3554\n",
      "Epoch [1240/2000], Loss: 0.3502\n",
      "Epoch [1241/2000], Loss: 0.3562\n",
      "Epoch [1242/2000], Loss: 0.3581\n",
      "Epoch [1243/2000], Loss: 0.3586\n",
      "Epoch [1244/2000], Loss: 0.3501\n",
      "Epoch [1245/2000], Loss: 0.3457\n",
      "Epoch [1246/2000], Loss: 0.3455\n",
      "Epoch [1247/2000], Loss: 0.3418\n",
      "Epoch [1248/2000], Loss: 0.3487\n",
      "Epoch [1249/2000], Loss: 0.3474\n",
      "Epoch [1250/2000], Loss: 0.3476\n",
      "Epoch [1251/2000], Loss: 0.3542\n",
      "Epoch [1252/2000], Loss: 0.3535\n",
      "Epoch [1253/2000], Loss: 0.3451\n",
      "Epoch [1254/2000], Loss: 0.3476\n",
      "Epoch [1255/2000], Loss: 0.3618\n",
      "Epoch [1256/2000], Loss: 0.3533\n",
      "Epoch [1257/2000], Loss: 0.3590\n",
      "Epoch [1258/2000], Loss: 0.3500\n",
      "Epoch [1259/2000], Loss: 0.3428\n",
      "Epoch [1260/2000], Loss: 0.3878\n",
      "Epoch [1261/2000], Loss: 0.3989\n",
      "Epoch [1262/2000], Loss: 0.3676\n",
      "Epoch [1263/2000], Loss: 0.3513\n",
      "Epoch [1264/2000], Loss: 0.3438\n",
      "Epoch [1265/2000], Loss: 0.3429\n",
      "Epoch [1266/2000], Loss: 0.3425\n",
      "Epoch [1267/2000], Loss: 0.3401\n",
      "Epoch [1268/2000], Loss: 0.3423\n",
      "Epoch [1269/2000], Loss: 0.3377\n",
      "Epoch [1270/2000], Loss: 0.3389\n",
      "Epoch [1271/2000], Loss: 0.3397\n",
      "Epoch [1272/2000], Loss: 0.3407\n",
      "Epoch [1273/2000], Loss: 0.3430\n",
      "Epoch [1274/2000], Loss: 0.3607\n",
      "Epoch [1275/2000], Loss: 0.3669\n",
      "Epoch [1276/2000], Loss: 0.3583\n",
      "Epoch [1277/2000], Loss: 0.3479\n",
      "Epoch [1278/2000], Loss: 0.3421\n",
      "Epoch [1279/2000], Loss: 0.3395\n",
      "Epoch [1280/2000], Loss: 0.3370\n",
      "Epoch [1281/2000], Loss: 0.3384\n",
      "Epoch [1282/2000], Loss: 0.3381\n",
      "Epoch [1283/2000], Loss: 0.3443\n",
      "Epoch [1284/2000], Loss: 0.3437\n",
      "Epoch [1285/2000], Loss: 0.3474\n",
      "Epoch [1286/2000], Loss: 0.3535\n",
      "Epoch [1287/2000], Loss: 0.3474\n",
      "Epoch [1288/2000], Loss: 0.3465\n",
      "Epoch [1289/2000], Loss: 0.3451\n",
      "Epoch [1290/2000], Loss: 0.3440\n",
      "Epoch [1291/2000], Loss: 0.3513\n",
      "Epoch [1292/2000], Loss: 0.3662\n",
      "Epoch [1293/2000], Loss: 0.3554\n",
      "Epoch [1294/2000], Loss: 0.3514\n",
      "Epoch [1295/2000], Loss: 0.3397\n",
      "Epoch [1296/2000], Loss: 0.3458\n",
      "Epoch [1297/2000], Loss: 0.3570\n",
      "Epoch [1298/2000], Loss: 0.3510\n",
      "Epoch [1299/2000], Loss: 0.3460\n",
      "Epoch [1300/2000], Loss: 0.3466\n",
      "Epoch [1301/2000], Loss: 0.3419\n",
      "Epoch [1302/2000], Loss: 0.3490\n",
      "Epoch [1303/2000], Loss: 0.3544\n",
      "Epoch [1304/2000], Loss: 0.3523\n",
      "Epoch [1305/2000], Loss: 0.3570\n",
      "Epoch [1306/2000], Loss: 0.3561\n",
      "Epoch [1307/2000], Loss: 0.3835\n",
      "Epoch [1308/2000], Loss: 0.3605\n",
      "Epoch [1309/2000], Loss: 0.3841\n",
      "Epoch [1310/2000], Loss: 0.3536\n",
      "Epoch [1311/2000], Loss: 0.3404\n",
      "Epoch [1312/2000], Loss: 0.3417\n",
      "Epoch [1313/2000], Loss: 0.3394\n",
      "Epoch [1314/2000], Loss: 0.3388\n",
      "Epoch [1315/2000], Loss: 0.3363\n",
      "Epoch [1316/2000], Loss: 0.3368\n",
      "Epoch [1317/2000], Loss: 0.3344\n",
      "Epoch [1318/2000], Loss: 0.3360\n",
      "Epoch [1319/2000], Loss: 0.3350\n",
      "Epoch [1320/2000], Loss: 0.3350\n",
      "Epoch [1321/2000], Loss: 0.3405\n",
      "Epoch [1322/2000], Loss: 0.3405\n",
      "Epoch [1323/2000], Loss: 0.3502\n",
      "Epoch [1324/2000], Loss: 0.3779\n",
      "Epoch [1325/2000], Loss: 0.3899\n",
      "Epoch [1326/2000], Loss: 0.3515\n",
      "Epoch [1327/2000], Loss: 0.3458\n",
      "Epoch [1328/2000], Loss: 0.3393\n",
      "Epoch [1329/2000], Loss: 0.3367\n",
      "Epoch [1330/2000], Loss: 0.3375\n",
      "Epoch [1331/2000], Loss: 0.3359\n",
      "Epoch [1332/2000], Loss: 0.3345\n",
      "Epoch [1333/2000], Loss: 0.3353\n",
      "Epoch [1334/2000], Loss: 0.3367\n",
      "Epoch [1335/2000], Loss: 0.3397\n",
      "Epoch [1336/2000], Loss: 0.3433\n",
      "Epoch [1337/2000], Loss: 0.3552\n",
      "Epoch [1338/2000], Loss: 0.4264\n",
      "Epoch [1339/2000], Loss: 0.3787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1340/2000], Loss: 0.3510\n",
      "Epoch [1341/2000], Loss: 0.3460\n",
      "Epoch [1342/2000], Loss: 0.3447\n",
      "Epoch [1343/2000], Loss: 0.3455\n",
      "Epoch [1344/2000], Loss: 0.3394\n",
      "Epoch [1345/2000], Loss: 0.3378\n",
      "Epoch [1346/2000], Loss: 0.3386\n",
      "Epoch [1347/2000], Loss: 0.3417\n",
      "Epoch [1348/2000], Loss: 0.3392\n",
      "Epoch [1349/2000], Loss: 0.3421\n",
      "Epoch [1350/2000], Loss: 0.3476\n",
      "Epoch [1351/2000], Loss: 0.3511\n",
      "Epoch [1352/2000], Loss: 0.3418\n",
      "Epoch [1353/2000], Loss: 0.3421\n",
      "Epoch [1354/2000], Loss: 0.3397\n",
      "Epoch [1355/2000], Loss: 0.3403\n",
      "Epoch [1356/2000], Loss: 0.3376\n",
      "Epoch [1357/2000], Loss: 0.3421\n",
      "Epoch [1358/2000], Loss: 0.3562\n",
      "Epoch [1359/2000], Loss: 0.3638\n",
      "Epoch [1360/2000], Loss: 0.3500\n",
      "Epoch [1361/2000], Loss: 0.3459\n",
      "Epoch [1362/2000], Loss: 0.3417\n",
      "Epoch [1363/2000], Loss: 0.3379\n",
      "Epoch [1364/2000], Loss: 0.3378\n",
      "Epoch [1365/2000], Loss: 0.3377\n",
      "Epoch [1366/2000], Loss: 0.3415\n",
      "Epoch [1367/2000], Loss: 0.3534\n",
      "Epoch [1368/2000], Loss: 0.3510\n",
      "Epoch [1369/2000], Loss: 0.3553\n",
      "Epoch [1370/2000], Loss: 0.3332\n",
      "Epoch [1371/2000], Loss: 0.3303\n",
      "Epoch [1372/2000], Loss: 0.3293\n",
      "Epoch [1373/2000], Loss: 0.3293\n",
      "Epoch [1374/2000], Loss: 0.3280\n",
      "Epoch [1375/2000], Loss: 0.3279\n",
      "Epoch [1376/2000], Loss: 0.3279\n",
      "Epoch [1377/2000], Loss: 0.3268\n",
      "Epoch [1378/2000], Loss: 0.3271\n",
      "Epoch [1379/2000], Loss: 0.3282\n",
      "Epoch [1380/2000], Loss: 0.3305\n",
      "Epoch [1381/2000], Loss: 0.3310\n",
      "Epoch [1382/2000], Loss: 0.3380\n",
      "Epoch [1383/2000], Loss: 0.3343\n",
      "Epoch [1384/2000], Loss: 0.3318\n",
      "Epoch [1385/2000], Loss: 0.3327\n",
      "Epoch [1386/2000], Loss: 0.3321\n",
      "Epoch [1387/2000], Loss: 0.3292\n",
      "Epoch [1388/2000], Loss: 0.3274\n",
      "Epoch [1389/2000], Loss: 0.3270\n",
      "Epoch [1390/2000], Loss: 0.3274\n",
      "Epoch [1391/2000], Loss: 0.3267\n",
      "Epoch [1392/2000], Loss: 0.3274\n",
      "Epoch [1393/2000], Loss: 0.3317\n",
      "Epoch [1394/2000], Loss: 0.3304\n",
      "Epoch [1395/2000], Loss: 0.3349\n",
      "Epoch [1396/2000], Loss: 0.3345\n",
      "Epoch [1397/2000], Loss: 0.3384\n",
      "Epoch [1398/2000], Loss: 0.3360\n",
      "Epoch [1399/2000], Loss: 0.3324\n",
      "Epoch [1400/2000], Loss: 0.3297\n",
      "Epoch [1401/2000], Loss: 0.3277\n",
      "Epoch [1402/2000], Loss: 0.3326\n",
      "Epoch [1403/2000], Loss: 0.3287\n",
      "Epoch [1404/2000], Loss: 0.3309\n",
      "Epoch [1405/2000], Loss: 0.3272\n",
      "Epoch [1406/2000], Loss: 0.3266\n",
      "Epoch [1407/2000], Loss: 0.3273\n",
      "Epoch [1408/2000], Loss: 0.3296\n",
      "Epoch [1409/2000], Loss: 0.3311\n",
      "Epoch [1410/2000], Loss: 0.3342\n",
      "Epoch [1411/2000], Loss: 0.3369\n",
      "Epoch [1412/2000], Loss: 0.3335\n",
      "Epoch [1413/2000], Loss: 0.3342\n",
      "Epoch [1414/2000], Loss: 0.3284\n",
      "Epoch [1415/2000], Loss: 0.3293\n",
      "Epoch [1416/2000], Loss: 0.3278\n",
      "Epoch [1417/2000], Loss: 0.3308\n",
      "Epoch [1418/2000], Loss: 0.3286\n",
      "Epoch [1419/2000], Loss: 0.3284\n",
      "Epoch [1420/2000], Loss: 0.3273\n",
      "Epoch [1421/2000], Loss: 0.3283\n",
      "Epoch [1422/2000], Loss: 0.3264\n",
      "Epoch [1423/2000], Loss: 0.3297\n",
      "Epoch [1424/2000], Loss: 0.3308\n",
      "Epoch [1425/2000], Loss: 0.3311\n",
      "Epoch [1426/2000], Loss: 0.3342\n",
      "Epoch [1427/2000], Loss: 0.3341\n",
      "Epoch [1428/2000], Loss: 0.3304\n",
      "Epoch [1429/2000], Loss: 0.3286\n",
      "Epoch [1430/2000], Loss: 0.3284\n",
      "Epoch [1431/2000], Loss: 0.3327\n",
      "Epoch [1432/2000], Loss: 0.3329\n",
      "Epoch [1433/2000], Loss: 0.3299\n",
      "Epoch [1434/2000], Loss: 0.3301\n",
      "Epoch [1435/2000], Loss: 0.3307\n",
      "Epoch [1436/2000], Loss: 0.3276\n",
      "Epoch [1437/2000], Loss: 0.3295\n",
      "Epoch [1438/2000], Loss: 0.3296\n",
      "Epoch [1439/2000], Loss: 0.3301\n",
      "Epoch [1440/2000], Loss: 0.3290\n",
      "Epoch [1441/2000], Loss: 0.3286\n",
      "Epoch [1442/2000], Loss: 0.3271\n",
      "Epoch [1443/2000], Loss: 0.3269\n",
      "Epoch [1444/2000], Loss: 0.3270\n",
      "Epoch [1445/2000], Loss: 0.3263\n",
      "Epoch [1446/2000], Loss: 0.3268\n",
      "Epoch [1447/2000], Loss: 0.3291\n",
      "Epoch [1448/2000], Loss: 0.3293\n",
      "Epoch [1449/2000], Loss: 0.3311\n",
      "Epoch [1450/2000], Loss: 0.3295\n",
      "Epoch [1451/2000], Loss: 0.3282\n",
      "Epoch [1452/2000], Loss: 0.3296\n",
      "Epoch [1453/2000], Loss: 0.3310\n",
      "Epoch [1454/2000], Loss: 0.3317\n",
      "Epoch [1455/2000], Loss: 0.3352\n",
      "Epoch [1456/2000], Loss: 0.3314\n",
      "Epoch [1457/2000], Loss: 0.3294\n",
      "Epoch [1458/2000], Loss: 0.3255\n",
      "Epoch [1459/2000], Loss: 0.3328\n",
      "Epoch [1460/2000], Loss: 0.3307\n",
      "Epoch [1461/2000], Loss: 0.3344\n",
      "Epoch [1462/2000], Loss: 0.3279\n",
      "Epoch [1463/2000], Loss: 0.3307\n",
      "Epoch [1464/2000], Loss: 0.3298\n",
      "Epoch [1465/2000], Loss: 0.3342\n",
      "Epoch [1466/2000], Loss: 0.3499\n",
      "Epoch [1467/2000], Loss: 0.3362\n",
      "Epoch [1468/2000], Loss: 0.3291\n",
      "Epoch [1469/2000], Loss: 0.3254\n",
      "Epoch [1470/2000], Loss: 0.3246\n",
      "Epoch [1471/2000], Loss: 0.3245\n",
      "Epoch [1472/2000], Loss: 0.3235\n",
      "Epoch [1473/2000], Loss: 0.3237\n",
      "Epoch [1474/2000], Loss: 0.3236\n",
      "Epoch [1475/2000], Loss: 0.3228\n",
      "Epoch [1476/2000], Loss: 0.3237\n",
      "Epoch [1477/2000], Loss: 0.3242\n",
      "Epoch [1478/2000], Loss: 0.3231\n",
      "Epoch [1479/2000], Loss: 0.3233\n",
      "Epoch [1480/2000], Loss: 0.3261\n",
      "Epoch [1481/2000], Loss: 0.3296\n",
      "Epoch [1482/2000], Loss: 0.3317\n",
      "Epoch [1483/2000], Loss: 0.3347\n",
      "Epoch [1484/2000], Loss: 0.3310\n",
      "Epoch [1485/2000], Loss: 0.3356\n",
      "Epoch [1486/2000], Loss: 0.3291\n",
      "Epoch [1487/2000], Loss: 0.3306\n",
      "Epoch [1488/2000], Loss: 0.3274\n",
      "Epoch [1489/2000], Loss: 0.3273\n",
      "Epoch [1490/2000], Loss: 0.3249\n",
      "Epoch [1491/2000], Loss: 0.3245\n",
      "Epoch [1492/2000], Loss: 0.3259\n",
      "Epoch [1493/2000], Loss: 0.3243\n",
      "Epoch [1494/2000], Loss: 0.3285\n",
      "Epoch [1495/2000], Loss: 0.3313\n",
      "Epoch [1496/2000], Loss: 0.3297\n",
      "Epoch [1497/2000], Loss: 0.3299\n",
      "Epoch [1498/2000], Loss: 0.3289\n",
      "Epoch [1499/2000], Loss: 0.3288\n",
      "Epoch [1500/2000], Loss: 0.3282\n",
      "Epoch [1501/2000], Loss: 0.3302\n",
      "Epoch [1502/2000], Loss: 0.3316\n",
      "Epoch [1503/2000], Loss: 0.3318\n",
      "Epoch [1504/2000], Loss: 0.3273\n",
      "Epoch [1505/2000], Loss: 0.3264\n",
      "Epoch [1506/2000], Loss: 0.3272\n",
      "Epoch [1507/2000], Loss: 0.3249\n",
      "Epoch [1508/2000], Loss: 0.3256\n",
      "Epoch [1509/2000], Loss: 0.3266\n",
      "Epoch [1510/2000], Loss: 0.3264\n",
      "Epoch [1511/2000], Loss: 0.3275\n",
      "Epoch [1512/2000], Loss: 0.3277\n",
      "Epoch [1513/2000], Loss: 0.3276\n",
      "Epoch [1514/2000], Loss: 0.3310\n",
      "Epoch [1515/2000], Loss: 0.3330\n",
      "Epoch [1516/2000], Loss: 0.3281\n",
      "Epoch [1517/2000], Loss: 0.3255\n",
      "Epoch [1518/2000], Loss: 0.3254\n",
      "Epoch [1519/2000], Loss: 0.3241\n",
      "Epoch [1520/2000], Loss: 0.3246\n",
      "Epoch [1521/2000], Loss: 0.3261\n",
      "Epoch [1522/2000], Loss: 0.3242\n",
      "Epoch [1523/2000], Loss: 0.3247\n",
      "Epoch [1524/2000], Loss: 0.3268\n",
      "Epoch [1525/2000], Loss: 0.3278\n",
      "Epoch [1526/2000], Loss: 0.3318\n",
      "Epoch [1527/2000], Loss: 0.3292\n",
      "Epoch [1528/2000], Loss: 0.3207\n",
      "Epoch [1529/2000], Loss: 0.3196\n",
      "Epoch [1530/2000], Loss: 0.3197\n",
      "Epoch [1531/2000], Loss: 0.3196\n",
      "Epoch [1532/2000], Loss: 0.3205\n",
      "Epoch [1533/2000], Loss: 0.3203\n",
      "Epoch [1534/2000], Loss: 0.3196\n",
      "Epoch [1535/2000], Loss: 0.3195\n",
      "Epoch [1536/2000], Loss: 0.3193\n",
      "Epoch [1537/2000], Loss: 0.3192\n",
      "Epoch [1538/2000], Loss: 0.3193\n",
      "Epoch [1539/2000], Loss: 0.3191\n",
      "Epoch [1540/2000], Loss: 0.3189\n",
      "Epoch [1541/2000], Loss: 0.3191\n",
      "Epoch [1542/2000], Loss: 0.3191\n",
      "Epoch [1543/2000], Loss: 0.3189\n",
      "Epoch [1544/2000], Loss: 0.3190\n",
      "Epoch [1545/2000], Loss: 0.3187\n",
      "Epoch [1546/2000], Loss: 0.3188\n",
      "Epoch [1547/2000], Loss: 0.3193\n",
      "Epoch [1548/2000], Loss: 0.3189\n",
      "Epoch [1549/2000], Loss: 0.3193\n",
      "Epoch [1550/2000], Loss: 0.3201\n",
      "Epoch [1551/2000], Loss: 0.3197\n",
      "Epoch [1552/2000], Loss: 0.3221\n",
      "Epoch [1553/2000], Loss: 0.3224\n",
      "Epoch [1554/2000], Loss: 0.3220\n",
      "Epoch [1555/2000], Loss: 0.3223\n",
      "Epoch [1556/2000], Loss: 0.3210\n",
      "Epoch [1557/2000], Loss: 0.3201\n",
      "Epoch [1558/2000], Loss: 0.3198\n",
      "Epoch [1559/2000], Loss: 0.3200\n",
      "Epoch [1560/2000], Loss: 0.3195\n",
      "Epoch [1561/2000], Loss: 0.3193\n",
      "Epoch [1562/2000], Loss: 0.3195\n",
      "Epoch [1563/2000], Loss: 0.3187\n",
      "Epoch [1564/2000], Loss: 0.3194\n",
      "Epoch [1565/2000], Loss: 0.3190\n",
      "Epoch [1566/2000], Loss: 0.3196\n",
      "Epoch [1567/2000], Loss: 0.3286\n",
      "Epoch [1568/2000], Loss: 0.3199\n",
      "Epoch [1569/2000], Loss: 0.3188\n",
      "Epoch [1570/2000], Loss: 0.3197\n",
      "Epoch [1571/2000], Loss: 0.3191\n",
      "Epoch [1572/2000], Loss: 0.3202\n",
      "Epoch [1573/2000], Loss: 0.3222\n",
      "Epoch [1574/2000], Loss: 0.3223\n",
      "Epoch [1575/2000], Loss: 0.3209\n",
      "Epoch [1576/2000], Loss: 0.3206\n",
      "Epoch [1577/2000], Loss: 0.3216\n",
      "Epoch [1578/2000], Loss: 0.3201\n",
      "Epoch [1579/2000], Loss: 0.3199\n",
      "Epoch [1580/2000], Loss: 0.3194\n",
      "Epoch [1581/2000], Loss: 0.3192\n",
      "Epoch [1582/2000], Loss: 0.3185\n",
      "Epoch [1583/2000], Loss: 0.3188\n",
      "Epoch [1584/2000], Loss: 0.3188\n",
      "Epoch [1585/2000], Loss: 0.3194\n",
      "Epoch [1586/2000], Loss: 0.3218\n",
      "Epoch [1587/2000], Loss: 0.3195\n",
      "Epoch [1588/2000], Loss: 0.3190\n",
      "Epoch [1589/2000], Loss: 0.3193\n",
      "Epoch [1590/2000], Loss: 0.3192\n",
      "Epoch [1591/2000], Loss: 0.3198\n",
      "Epoch [1592/2000], Loss: 0.3202\n",
      "Epoch [1593/2000], Loss: 0.3209\n",
      "Epoch [1594/2000], Loss: 0.3223\n",
      "Epoch [1595/2000], Loss: 0.3241\n",
      "Epoch [1596/2000], Loss: 0.3227\n",
      "Epoch [1597/2000], Loss: 0.3204\n",
      "Epoch [1598/2000], Loss: 0.3196\n",
      "Epoch [1599/2000], Loss: 0.3187\n",
      "Epoch [1600/2000], Loss: 0.3182\n",
      "Epoch [1601/2000], Loss: 0.3186\n",
      "Epoch [1602/2000], Loss: 0.3183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1603/2000], Loss: 0.3182\n",
      "Epoch [1604/2000], Loss: 0.3179\n",
      "Epoch [1605/2000], Loss: 0.3185\n",
      "Epoch [1606/2000], Loss: 0.3180\n",
      "Epoch [1607/2000], Loss: 0.3179\n",
      "Epoch [1608/2000], Loss: 0.3194\n",
      "Epoch [1609/2000], Loss: 0.3188\n",
      "Epoch [1610/2000], Loss: 0.3193\n",
      "Epoch [1611/2000], Loss: 0.3193\n",
      "Epoch [1612/2000], Loss: 0.3210\n",
      "Epoch [1613/2000], Loss: 0.3226\n",
      "Epoch [1614/2000], Loss: 0.3239\n",
      "Epoch [1615/2000], Loss: 0.3211\n",
      "Epoch [1616/2000], Loss: 0.3205\n",
      "Epoch [1617/2000], Loss: 0.3186\n",
      "Epoch [1618/2000], Loss: 0.3192\n",
      "Epoch [1619/2000], Loss: 0.3232\n",
      "Epoch [1620/2000], Loss: 0.3192\n",
      "Epoch [1621/2000], Loss: 0.3181\n",
      "Epoch [1622/2000], Loss: 0.3180\n",
      "Epoch [1623/2000], Loss: 0.3183\n",
      "Epoch [1624/2000], Loss: 0.3178\n",
      "Epoch [1625/2000], Loss: 0.3184\n",
      "Epoch [1626/2000], Loss: 0.3186\n",
      "Epoch [1627/2000], Loss: 0.3177\n",
      "Epoch [1628/2000], Loss: 0.3193\n",
      "Epoch [1629/2000], Loss: 0.3206\n",
      "Epoch [1630/2000], Loss: 0.3223\n",
      "Epoch [1631/2000], Loss: 0.3207\n",
      "Epoch [1632/2000], Loss: 0.3330\n",
      "Epoch [1633/2000], Loss: 0.3206\n",
      "Epoch [1634/2000], Loss: 0.3190\n",
      "Epoch [1635/2000], Loss: 0.3188\n",
      "Epoch [1636/2000], Loss: 0.3175\n",
      "Epoch [1637/2000], Loss: 0.3171\n",
      "Epoch [1638/2000], Loss: 0.3173\n",
      "Epoch [1639/2000], Loss: 0.3175\n",
      "Epoch [1640/2000], Loss: 0.3173\n",
      "Epoch [1641/2000], Loss: 0.3176\n",
      "Epoch [1642/2000], Loss: 0.3174\n",
      "Epoch [1643/2000], Loss: 0.3175\n",
      "Epoch [1644/2000], Loss: 0.3178\n",
      "Epoch [1645/2000], Loss: 0.3177\n",
      "Epoch [1646/2000], Loss: 0.3179\n",
      "Epoch [1647/2000], Loss: 0.3172\n",
      "Epoch [1648/2000], Loss: 0.3174\n",
      "Epoch [1649/2000], Loss: 0.3184\n",
      "Epoch [1650/2000], Loss: 0.3210\n",
      "Epoch [1651/2000], Loss: 0.3228\n",
      "Epoch [1652/2000], Loss: 0.3209\n",
      "Epoch [1653/2000], Loss: 0.3202\n",
      "Epoch [1654/2000], Loss: 0.3193\n",
      "Epoch [1655/2000], Loss: 0.3207\n",
      "Epoch [1656/2000], Loss: 0.3180\n",
      "Epoch [1657/2000], Loss: 0.3176\n",
      "Epoch [1658/2000], Loss: 0.3158\n",
      "Epoch [1659/2000], Loss: 0.3160\n",
      "Epoch [1660/2000], Loss: 0.3156\n",
      "Epoch [1661/2000], Loss: 0.3153\n",
      "Epoch [1662/2000], Loss: 0.3160\n",
      "Epoch [1663/2000], Loss: 0.3170\n",
      "Epoch [1664/2000], Loss: 0.3183\n",
      "Epoch [1665/2000], Loss: 0.3192\n",
      "Epoch [1666/2000], Loss: 0.3182\n",
      "Epoch [1667/2000], Loss: 0.3170\n",
      "Epoch [1668/2000], Loss: 0.3171\n",
      "Epoch [1669/2000], Loss: 0.3162\n",
      "Epoch [1670/2000], Loss: 0.3176\n",
      "Epoch [1671/2000], Loss: 0.3164\n",
      "Epoch [1672/2000], Loss: 0.3173\n",
      "Epoch [1673/2000], Loss: 0.3170\n",
      "Epoch [1674/2000], Loss: 0.3180\n",
      "Epoch [1675/2000], Loss: 0.3197\n",
      "Epoch [1676/2000], Loss: 0.3194\n",
      "Epoch [1677/2000], Loss: 0.3185\n",
      "Epoch [1678/2000], Loss: 0.3176\n",
      "Epoch [1679/2000], Loss: 0.3166\n",
      "Epoch [1680/2000], Loss: 0.3173\n",
      "Epoch [1681/2000], Loss: 0.3166\n",
      "Epoch [1682/2000], Loss: 0.3163\n",
      "Epoch [1683/2000], Loss: 0.3163\n",
      "Epoch [1684/2000], Loss: 0.3161\n",
      "Epoch [1685/2000], Loss: 0.3171\n",
      "Epoch [1686/2000], Loss: 0.3168\n",
      "Epoch [1687/2000], Loss: 0.3171\n",
      "Epoch [1688/2000], Loss: 0.3172\n",
      "Epoch [1689/2000], Loss: 0.3174\n",
      "Epoch [1690/2000], Loss: 0.3177\n",
      "Epoch [1691/2000], Loss: 0.3169\n",
      "Epoch [1692/2000], Loss: 0.3169\n",
      "Epoch [1693/2000], Loss: 0.3160\n",
      "Epoch [1694/2000], Loss: 0.3164\n",
      "Epoch [1695/2000], Loss: 0.3162\n",
      "Epoch [1696/2000], Loss: 0.3164\n",
      "Epoch [1697/2000], Loss: 0.3170\n",
      "Epoch [1698/2000], Loss: 0.3181\n",
      "Epoch [1699/2000], Loss: 0.3174\n",
      "Epoch [1700/2000], Loss: 0.3198\n",
      "Epoch [1701/2000], Loss: 0.3179\n",
      "Epoch [1702/2000], Loss: 0.3180\n",
      "Epoch [1703/2000], Loss: 0.3166\n",
      "Epoch [1704/2000], Loss: 0.3170\n",
      "Epoch [1705/2000], Loss: 0.3158\n",
      "Epoch [1706/2000], Loss: 0.3167\n",
      "Epoch [1707/2000], Loss: 0.3156\n",
      "Epoch [1708/2000], Loss: 0.3179\n",
      "Epoch [1709/2000], Loss: 0.3169\n",
      "Epoch [1710/2000], Loss: 0.3170\n",
      "Epoch [1711/2000], Loss: 0.3177\n",
      "Epoch [1712/2000], Loss: 0.3168\n",
      "Epoch [1713/2000], Loss: 0.3156\n",
      "Epoch [1714/2000], Loss: 0.3133\n",
      "Epoch [1715/2000], Loss: 0.3137\n",
      "Epoch [1716/2000], Loss: 0.3134\n",
      "Epoch [1717/2000], Loss: 0.3136\n",
      "Epoch [1718/2000], Loss: 0.3135\n",
      "Epoch [1719/2000], Loss: 0.3134\n",
      "Epoch [1720/2000], Loss: 0.3133\n",
      "Epoch [1721/2000], Loss: 0.3133\n",
      "Epoch [1722/2000], Loss: 0.3132\n",
      "Epoch [1723/2000], Loss: 0.3131\n",
      "Epoch [1724/2000], Loss: 0.3130\n",
      "Epoch [1725/2000], Loss: 0.3132\n",
      "Epoch [1726/2000], Loss: 0.3129\n",
      "Epoch [1727/2000], Loss: 0.3133\n",
      "Epoch [1728/2000], Loss: 0.3130\n",
      "Epoch [1729/2000], Loss: 0.3133\n",
      "Epoch [1730/2000], Loss: 0.3132\n",
      "Epoch [1731/2000], Loss: 0.3137\n",
      "Epoch [1732/2000], Loss: 0.3131\n",
      "Epoch [1733/2000], Loss: 0.3137\n",
      "Epoch [1734/2000], Loss: 0.3131\n",
      "Epoch [1735/2000], Loss: 0.3130\n",
      "Epoch [1736/2000], Loss: 0.3132\n",
      "Epoch [1737/2000], Loss: 0.3130\n",
      "Epoch [1738/2000], Loss: 0.3131\n",
      "Epoch [1739/2000], Loss: 0.3142\n",
      "Epoch [1740/2000], Loss: 0.3139\n",
      "Epoch [1741/2000], Loss: 0.3138\n",
      "Epoch [1742/2000], Loss: 0.3140\n",
      "Epoch [1743/2000], Loss: 0.3147\n",
      "Epoch [1744/2000], Loss: 0.3139\n",
      "Epoch [1745/2000], Loss: 0.3139\n",
      "Epoch [1746/2000], Loss: 0.3132\n",
      "Epoch [1747/2000], Loss: 0.3136\n",
      "Epoch [1748/2000], Loss: 0.3131\n",
      "Epoch [1749/2000], Loss: 0.3134\n",
      "Epoch [1750/2000], Loss: 0.3132\n",
      "Epoch [1751/2000], Loss: 0.3130\n",
      "Epoch [1752/2000], Loss: 0.3131\n",
      "Epoch [1753/2000], Loss: 0.3128\n",
      "Epoch [1754/2000], Loss: 0.3132\n",
      "Epoch [1755/2000], Loss: 0.3132\n",
      "Epoch [1756/2000], Loss: 0.3135\n",
      "Epoch [1757/2000], Loss: 0.3133\n",
      "Epoch [1758/2000], Loss: 0.3133\n",
      "Epoch [1759/2000], Loss: 0.3134\n",
      "Epoch [1760/2000], Loss: 0.3136\n",
      "Epoch [1761/2000], Loss: 0.3137\n",
      "Epoch [1762/2000], Loss: 0.3137\n",
      "Epoch [1763/2000], Loss: 0.3154\n",
      "Epoch [1764/2000], Loss: 0.3135\n",
      "Epoch [1765/2000], Loss: 0.3135\n",
      "Epoch [1766/2000], Loss: 0.3129\n",
      "Epoch [1767/2000], Loss: 0.3131\n",
      "Epoch [1768/2000], Loss: 0.3130\n",
      "Epoch [1769/2000], Loss: 0.3131\n",
      "Epoch [1770/2000], Loss: 0.3130\n",
      "Epoch [1771/2000], Loss: 0.3129\n",
      "Epoch [1772/2000], Loss: 0.3131\n",
      "Epoch [1773/2000], Loss: 0.3127\n",
      "Epoch [1774/2000], Loss: 0.3128\n",
      "Epoch [1775/2000], Loss: 0.3126\n",
      "Epoch [1776/2000], Loss: 0.3132\n",
      "Epoch [1777/2000], Loss: 0.3135\n",
      "Epoch [1778/2000], Loss: 0.3138\n",
      "Epoch [1779/2000], Loss: 0.3134\n",
      "Epoch [1780/2000], Loss: 0.3138\n",
      "Epoch [1781/2000], Loss: 0.3136\n",
      "Epoch [1782/2000], Loss: 0.3140\n",
      "Epoch [1783/2000], Loss: 0.3141\n",
      "Epoch [1784/2000], Loss: 0.3132\n",
      "Epoch [1785/2000], Loss: 0.3133\n",
      "Epoch [1786/2000], Loss: 0.3128\n",
      "Epoch [1787/2000], Loss: 0.3131\n",
      "Epoch [1788/2000], Loss: 0.3128\n",
      "Epoch [1789/2000], Loss: 0.3130\n",
      "Epoch [1790/2000], Loss: 0.3127\n",
      "Epoch [1791/2000], Loss: 0.3129\n",
      "Epoch [1792/2000], Loss: 0.3140\n",
      "Epoch [1793/2000], Loss: 0.3130\n",
      "Epoch [1794/2000], Loss: 0.3137\n",
      "Epoch [1795/2000], Loss: 0.3133\n",
      "Epoch [1796/2000], Loss: 0.3130\n",
      "Epoch [1797/2000], Loss: 0.3141\n",
      "Epoch [1798/2000], Loss: 0.3132\n",
      "Epoch [1799/2000], Loss: 0.3154\n",
      "Epoch [1800/2000], Loss: 0.3139\n",
      "Epoch [1801/2000], Loss: 0.3149\n",
      "Epoch [1802/2000], Loss: 0.3130\n",
      "Epoch [1803/2000], Loss: 0.3134\n",
      "Epoch [1804/2000], Loss: 0.3131\n",
      "Epoch [1805/2000], Loss: 0.3126\n",
      "Epoch [1806/2000], Loss: 0.3129\n",
      "Epoch [1807/2000], Loss: 0.3128\n",
      "Epoch [1808/2000], Loss: 0.3130\n",
      "Epoch [1809/2000], Loss: 0.3134\n",
      "Epoch [1810/2000], Loss: 0.3134\n",
      "Epoch [1811/2000], Loss: 0.3135\n",
      "Epoch [1812/2000], Loss: 0.3136\n",
      "Epoch [1813/2000], Loss: 0.3132\n",
      "Epoch [1814/2000], Loss: 0.3132\n",
      "Epoch [1815/2000], Loss: 0.3131\n",
      "Epoch [1816/2000], Loss: 0.3129\n",
      "Epoch [1817/2000], Loss: 0.3130\n",
      "Epoch [1818/2000], Loss: 0.3130\n",
      "Epoch [1819/2000], Loss: 0.3124\n",
      "Epoch [1820/2000], Loss: 0.3127\n",
      "Epoch [1821/2000], Loss: 0.3131\n",
      "Epoch [1822/2000], Loss: 0.3128\n",
      "Epoch [1823/2000], Loss: 0.3132\n",
      "Epoch [1824/2000], Loss: 0.3123\n",
      "Epoch [1825/2000], Loss: 0.3127\n",
      "Epoch [1826/2000], Loss: 0.3125\n",
      "Epoch [1827/2000], Loss: 0.3129\n",
      "Epoch [1828/2000], Loss: 0.3130\n",
      "Epoch [1829/2000], Loss: 0.3129\n",
      "Epoch [1830/2000], Loss: 0.3149\n",
      "Epoch [1831/2000], Loss: 0.3149\n",
      "Epoch [1832/2000], Loss: 0.3145\n",
      "Epoch [1833/2000], Loss: 0.3137\n",
      "Epoch [1834/2000], Loss: 0.3138\n",
      "Epoch [1835/2000], Loss: 0.3131\n",
      "Epoch [1836/2000], Loss: 0.3139\n",
      "Epoch [1837/2000], Loss: 0.3126\n",
      "Epoch [1838/2000], Loss: 0.3124\n",
      "Epoch [1839/2000], Loss: 0.3153\n",
      "Epoch [1840/2000], Loss: 0.3127\n",
      "Epoch [1841/2000], Loss: 0.3121\n",
      "Epoch [1842/2000], Loss: 0.3122\n",
      "Epoch [1843/2000], Loss: 0.3122\n",
      "Epoch [1844/2000], Loss: 0.3124\n",
      "Epoch [1845/2000], Loss: 0.3128\n",
      "Epoch [1846/2000], Loss: 0.3124\n",
      "Epoch [1847/2000], Loss: 0.3125\n",
      "Epoch [1848/2000], Loss: 0.3124\n",
      "Epoch [1849/2000], Loss: 0.3125\n",
      "Epoch [1850/2000], Loss: 0.3124\n",
      "Epoch [1851/2000], Loss: 0.3131\n",
      "Epoch [1852/2000], Loss: 0.3140\n",
      "Epoch [1853/2000], Loss: 0.3140\n",
      "Epoch [1854/2000], Loss: 0.3144\n",
      "Epoch [1855/2000], Loss: 0.3134\n",
      "Epoch [1856/2000], Loss: 0.3131\n",
      "Epoch [1857/2000], Loss: 0.3126\n",
      "Epoch [1858/2000], Loss: 0.3125\n",
      "Epoch [1859/2000], Loss: 0.3124\n",
      "Epoch [1860/2000], Loss: 0.3123\n",
      "Epoch [1861/2000], Loss: 0.3124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1862/2000], Loss: 0.3123\n",
      "Epoch [1863/2000], Loss: 0.3124\n",
      "Epoch [1864/2000], Loss: 0.3124\n",
      "Epoch [1865/2000], Loss: 0.3124\n",
      "Epoch [1866/2000], Loss: 0.3129\n",
      "Epoch [1867/2000], Loss: 0.3125\n",
      "Epoch [1868/2000], Loss: 0.3128\n",
      "Epoch [1869/2000], Loss: 0.3126\n",
      "Epoch [1870/2000], Loss: 0.3128\n",
      "Epoch [1871/2000], Loss: 0.3127\n",
      "Epoch [1872/2000], Loss: 0.3123\n",
      "Epoch [1873/2000], Loss: 0.3129\n",
      "Epoch [1874/2000], Loss: 0.3146\n",
      "Epoch [1875/2000], Loss: 0.3127\n",
      "Epoch [1876/2000], Loss: 0.3140\n",
      "Epoch [1877/2000], Loss: 0.3121\n",
      "Epoch [1878/2000], Loss: 0.3129\n",
      "Epoch [1879/2000], Loss: 0.3129\n",
      "Epoch [1880/2000], Loss: 0.3126\n",
      "Epoch [1881/2000], Loss: 0.3127\n",
      "Epoch [1882/2000], Loss: 0.3130\n",
      "Epoch [1883/2000], Loss: 0.3130\n",
      "Epoch [1884/2000], Loss: 0.3132\n",
      "Epoch [1885/2000], Loss: 0.3130\n",
      "Epoch [1886/2000], Loss: 0.3128\n",
      "Epoch [1887/2000], Loss: 0.3133\n",
      "Epoch [1888/2000], Loss: 0.3133\n",
      "Epoch [1889/2000], Loss: 0.3124\n",
      "Epoch [1890/2000], Loss: 0.3123\n",
      "Epoch [1891/2000], Loss: 0.3129\n",
      "Epoch [1892/2000], Loss: 0.3127\n",
      "Epoch [1893/2000], Loss: 0.3135\n",
      "Epoch [1894/2000], Loss: 0.3124\n",
      "Epoch [1895/2000], Loss: 0.3126\n",
      "Epoch [1896/2000], Loss: 0.3122\n",
      "Epoch [1897/2000], Loss: 0.3123\n",
      "Epoch [1898/2000], Loss: 0.3132\n",
      "Epoch [1899/2000], Loss: 0.3123\n",
      "Epoch [1900/2000], Loss: 0.3125\n",
      "Epoch [1901/2000], Loss: 0.3130\n",
      "Epoch [1902/2000], Loss: 0.3128\n",
      "Epoch [1903/2000], Loss: 0.3129\n",
      "Epoch [1904/2000], Loss: 0.3124\n",
      "Epoch [1905/2000], Loss: 0.3120\n",
      "Epoch [1906/2000], Loss: 0.3119\n",
      "Epoch [1907/2000], Loss: 0.3121\n",
      "Epoch [1908/2000], Loss: 0.3116\n",
      "Epoch [1909/2000], Loss: 0.3127\n",
      "Epoch [1910/2000], Loss: 0.3118\n",
      "Epoch [1911/2000], Loss: 0.3128\n",
      "Epoch [1912/2000], Loss: 0.3124\n",
      "Epoch [1913/2000], Loss: 0.3128\n",
      "Epoch [1914/2000], Loss: 0.3137\n",
      "Epoch [1915/2000], Loss: 0.3139\n",
      "Epoch [1916/2000], Loss: 0.3135\n",
      "Epoch [1917/2000], Loss: 0.3125\n",
      "Epoch [1918/2000], Loss: 0.3135\n",
      "Epoch [1919/2000], Loss: 0.3123\n",
      "Epoch [1920/2000], Loss: 0.3118\n",
      "Epoch [1921/2000], Loss: 0.3117\n",
      "Epoch [1922/2000], Loss: 0.3118\n",
      "Epoch [1923/2000], Loss: 0.3200\n",
      "Epoch [1924/2000], Loss: 0.3129\n",
      "Epoch [1925/2000], Loss: 0.3124\n",
      "Epoch [1926/2000], Loss: 0.3123\n",
      "Epoch [1927/2000], Loss: 0.3121\n",
      "Epoch [1928/2000], Loss: 0.3118\n",
      "Epoch [1929/2000], Loss: 0.3123\n",
      "Epoch [1930/2000], Loss: 0.3132\n",
      "Epoch [1931/2000], Loss: 0.3123\n",
      "Epoch [1932/2000], Loss: 0.3129\n",
      "Epoch [1933/2000], Loss: 0.3118\n",
      "Epoch [1934/2000], Loss: 0.3122\n",
      "Epoch [1935/2000], Loss: 0.3120\n",
      "Epoch [1936/2000], Loss: 0.3123\n",
      "Epoch [1937/2000], Loss: 0.3122\n",
      "Epoch [1938/2000], Loss: 0.3123\n",
      "Epoch [1939/2000], Loss: 0.3126\n",
      "Epoch [1940/2000], Loss: 0.3145\n",
      "Epoch [1941/2000], Loss: 0.3127\n",
      "Epoch [1942/2000], Loss: 0.3127\n",
      "Epoch [1943/2000], Loss: 0.3127\n",
      "Epoch [1944/2000], Loss: 0.3119\n",
      "Epoch [1945/2000], Loss: 0.3120\n",
      "Epoch [1946/2000], Loss: 0.3120\n",
      "Epoch [1947/2000], Loss: 0.3126\n",
      "Epoch [1948/2000], Loss: 0.3124\n",
      "Epoch [1949/2000], Loss: 0.3120\n",
      "Epoch [1950/2000], Loss: 0.3123\n",
      "Epoch [1951/2000], Loss: 0.3123\n",
      "Epoch [1952/2000], Loss: 0.3121\n",
      "Epoch [1953/2000], Loss: 0.3127\n",
      "Epoch [1954/2000], Loss: 0.3123\n",
      "Epoch [1955/2000], Loss: 0.3125\n",
      "Epoch [1956/2000], Loss: 0.3121\n",
      "Epoch [1957/2000], Loss: 0.3119\n",
      "Epoch [1958/2000], Loss: 0.3121\n",
      "Epoch [1959/2000], Loss: 0.3119\n",
      "Epoch [1960/2000], Loss: 0.3109\n",
      "Epoch [1961/2000], Loss: 0.3111\n",
      "Epoch [1962/2000], Loss: 0.3115\n",
      "Epoch [1963/2000], Loss: 0.3111\n",
      "Epoch [1964/2000], Loss: 0.3109\n",
      "Epoch [1965/2000], Loss: 0.3110\n",
      "Epoch [1966/2000], Loss: 0.3108\n",
      "Epoch [1967/2000], Loss: 0.3108\n",
      "Epoch [1968/2000], Loss: 0.3109\n",
      "Epoch [1969/2000], Loss: 0.3110\n",
      "Epoch [1970/2000], Loss: 0.3109\n",
      "Epoch [1971/2000], Loss: 0.3109\n",
      "Epoch [1972/2000], Loss: 0.3107\n",
      "Epoch [1973/2000], Loss: 0.3110\n",
      "Epoch [1974/2000], Loss: 0.3110\n",
      "Epoch [1975/2000], Loss: 0.3111\n",
      "Epoch [1976/2000], Loss: 0.3113\n",
      "Epoch [1977/2000], Loss: 0.3109\n",
      "Epoch [1978/2000], Loss: 0.3109\n",
      "Epoch [1979/2000], Loss: 0.3106\n",
      "Epoch [1980/2000], Loss: 0.3107\n",
      "Epoch [1981/2000], Loss: 0.3110\n",
      "Epoch [1982/2000], Loss: 0.3112\n",
      "Epoch [1983/2000], Loss: 0.3110\n",
      "Epoch [1984/2000], Loss: 0.3110\n",
      "Epoch [1985/2000], Loss: 0.3110\n",
      "Epoch [1986/2000], Loss: 0.3109\n",
      "Epoch [1987/2000], Loss: 0.3109\n",
      "Epoch [1988/2000], Loss: 0.3121\n",
      "Epoch [1989/2000], Loss: 0.3114\n",
      "Epoch [1990/2000], Loss: 0.3110\n",
      "Epoch [1991/2000], Loss: 0.3114\n",
      "Epoch [1992/2000], Loss: 0.3110\n",
      "Epoch [1993/2000], Loss: 0.3110\n",
      "Epoch [1994/2000], Loss: 0.3109\n",
      "Epoch [1995/2000], Loss: 0.3107\n",
      "Epoch [1996/2000], Loss: 0.3105\n",
      "Epoch [1997/2000], Loss: 0.3107\n",
      "Epoch [1998/2000], Loss: 0.3108\n",
      "Epoch [1999/2000], Loss: 0.3110\n",
      "Epoch [2000/2000], Loss: 0.3106\n"
     ]
    }
   ],
   "source": [
    "print(\"Start Training\")\n",
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    validation_loss = 0  # Reset validation loss at the beginning of each epoch\n",
    "    for batch in data_loader:\n",
    "        inputs = batch.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        validation_loss += loss.item()\n",
    "    \n",
    "    validation_loss /= len(data_loader)\n",
    "    scheduler.step(validation_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {validation_loss:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##  Testen an Anomalien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df  = pd.read_csv(\"./data/fan_anomaly_train.csv\", delimiter=\",\")\n",
    "df = df[df[\"anomaly\"] == 1.0]\n",
    "df = df.drop(\"id\", axis=1)\n",
    "df = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for the unusual dataset: 6.6374\n"
     ]
    }
   ],
   "source": [
    "your_data = torch.tensor(df) \n",
    "your_data = your_data.to(torch.float32)\n",
    "\n",
    "unusual_dataset = CustomDataset(your_data)\n",
    "unusual_data_loader = data.DataLoader(unusual_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "mse_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in unusual_data_loader:\n",
    "        inputs = batch.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        \n",
    "        mse_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "mse_loss /= num_batches\n",
    "print(f'MSE for the unusual dataset: {mse_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for entry 1: 1.5793\n",
      "MSE for entry 2: 1.7731\n",
      "MSE for entry 3: 1.3620\n",
      "MSE for entry 4: 1.5902\n",
      "MSE for entry 5: 3.4454\n",
      "MSE for entry 6: 1.9689\n",
      "MSE for entry 7: 1.7577\n",
      "MSE for entry 8: 1.4562\n"
     ]
    }
   ],
   "source": [
    "# Calculate per-entry MSE\n",
    "squared_errors = (inputs - outputs) ** 2\n",
    "mse_per_entry = squared_errors.mean(dim=1).detach().cpu().numpy()\n",
    "\n",
    "# Print MSE for each entry\n",
    "for idx, mse in enumerate(mse_per_entry):\n",
    "    print(f'MSE for entry {idx + 1}: {mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Einschub Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vielen Dank\n",
    "\n",
    "<center><img src=\"img/lernlandkarte.png\" width=80% class=\"stretch\"></center>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "FunML",
   "language": "python",
   "name": "funml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
